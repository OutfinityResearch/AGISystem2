<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>The Algorithmic Revolution: Probabilistic Sparsity â€” AGISystem2</title>
    <style>
      :root {
        --bg0: #f7fbff; --bg1: #fff7fb; --card: rgba(255, 255, 255, 0.78);
        --stroke: rgba(2, 6, 23, 0.16); --text: rgba(2, 6, 23, 0.92);
        --muted: rgba(2, 6, 23, 0.72); --accent: #0a7dff;
        --radius: 18px; --maxw: 1120px;
      }
      body { margin: 0; font-family: ui-sans-serif, system-ui, sans-serif; color: var(--text); background: linear-gradient(180deg, var(--bg0), var(--bg1)); }
      .shell { max-width: var(--maxw); margin: 0 auto; padding: 22px 18px 64px; }
      .topbar { display: flex; align-items: center; justify-content: space-between; padding: 14px; border: 1px solid var(--stroke); background: rgba(255, 255, 255, 0.86); backdrop-filter: blur(10px); border-radius: var(--radius); position: sticky; top: 14px; z-index: 20; }
      .pill { display: inline-flex; align-items: center; gap: 8px; padding: 9px 11px; border: 1px solid var(--stroke); border-radius: 999px; font-size: 13px; color: var(--text); text-decoration: none; }
      .hero { margin-top: 18px; padding: 32px; border: 1px solid var(--stroke); border-radius: var(--radius); background: white; }
      .stack { margin-top: 18px; padding: 32px; border: 1px solid var(--stroke); border-radius: var(--radius); background: white; line-height: 1.6; }
      h2 { color: var(--accent); margin-top: 24px; }
      p { margin-bottom: 16px; }
      ul { margin-bottom: 16px; padding-left: 20px; }
      li { margin-bottom: 8px; }
    </style>
  </head>
  <body>
    <div class="shell">
      <div class="topbar">
        <div><strong>AGISystem2 Research</strong></div>
        <div class="top-links">
          <a class="pill" href="../index.html">Home</a>
          <a class="pill" href="../research.html">Back</a>
        </div>
      </div>
      <header class="hero">
        <h1>The Algorithmic Revolution</h1>
        <p>Transitioning from Dense Matrix Multiplication to Probabilistic Sparsity.</p>
      </header>
      <main class="stack">
        <h2>1.1 The SLIDE Algorithm: Sub-Linear Deep Learning Engine</h2>
        <p>The SLIDE architecture, developed at Rice University, addresses the computational cost of neural network training by utilizing <strong><a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" target="_blank">Locality Sensitive Hashing (LSH)</a></strong>. It identifies active neurons for specific inputs without computing full layer activations.</p>
        <ul>
          <li><strong>O(1) Complexity:</strong> Aims for constant-time lookups rather than O(N) linear complexity.</li>
          <li><strong>CPU Optimization:</strong> Efficiently utilizes large L3 caches and branch prediction, which are less effective in GPU-centric SIMD architectures.</li>
        </ul>

        <h2>1.2 ThirdAI and Dynamic Sparsity</h2>
        <p><a href="https://www.thirdai.com/" target="_blank">ThirdAI</a> commercialized these concepts through the <a href="https://www.thirdai.com/bolt/" target="_blank">BOLT engine</a>. The objective is to enable training and fine-tuning of large-scale models directly on standard x86 and ARM CPUs.</p>
        <ul>
          <li><strong>Training Efficiency:</strong> Focus on dynamic sparsity during training, avoiding the need for high-bandwidth VRAM.</li>
          <li><strong>Privacy:</strong> On-premise deployment for sensitive medical or financial datasets.</li>
        </ul>

        <h2>1.3 Neural Magic: Sparsification and serving</h2>
        <p><a href="https://neuralmagic.com/" target="_blank">Neural Magic</a>, born from MIT, focuses on <strong>Sparsification</strong>, pruning networks by 80-90% without losing accuracy. Following their 2025 acquisition by Red Hat, they have pivoted to contributing these optimizations directly to <a href="https://github.com/vllm-project/vllm" target="_blank">vLLM</a> via the <a href="https://github.com/neuralmagic/nm-vllm" target="_blank">nm-vllm</a> project.</p>

        <h2>Alternative Sparse Methods</h2>
        <ul>
          <li><strong><a href="https://en.wikipedia.org/wiki/Random_projection" target="_blank">Random Projections</a>:</strong> Based on the Johnson-Lindenstrauss lemma, these techniques reduce data dimensionality while preserving distances, providing a CPU-friendly alternative to deep embedding layers.</li>
          <li><strong><a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Bloom Filters in ML</a>:</strong> Utilizing probabilistic data structures for ultra-fast, constant-time set membership tests in large-scale classification tasks.</li>
          <li><strong><a href="https://en.wikipedia.org/wiki/MinHash" target="_blank">MinHash</a>:</strong> A technique for estimating the similarity of sets, used in large-scale deduplication and clustering before neural processing.</li>
          <li><strong><a href="https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch" target="_blank">Count-Min Sketch</a>:</strong> A probabilistic data structure used for frequency estimation in data streams, useful for real-time feature selection on CPUs.</li>
        </ul>
      </main>
    </div>
  </body>
</html>