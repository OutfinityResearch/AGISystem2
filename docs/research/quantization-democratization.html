<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Democratization through Quantization: llama.cpp â€” AGISystem2</title>
    <style>
      :root {
        --bg0: #f7fbff; --bg1: #fff7fb; --stroke: rgba(2, 6, 23, 0.16);
        --text: rgba(2, 6, 23, 0.92); --muted: rgba(2, 6, 23, 0.72);
        --accent: #0a7dff; --radius: 18px; --maxw: 1120px;
      }
      body { margin: 0; font-family: ui-sans-serif, system-ui, sans-serif; color: var(--text); background: linear-gradient(180deg, var(--bg0), var(--bg1)); }
      .shell { max-width: var(--maxw); margin: 0 auto; padding: 22px 18px 64px; }
      .topbar { display: flex; align-items: center; justify-content: space-between; padding: 14px; border: 1px solid var(--stroke); background: rgba(255, 255, 255, 0.86); backdrop-filter: blur(10px); border-radius: var(--radius); position: sticky; top: 14px; z-index: 20; }
      .pill { display: inline-flex; align-items: center; gap: 8px; padding: 9px 11px; border: 1px solid var(--stroke); border-radius: 999px; font-size: 13px; color: var(--text); text-decoration: none; }
      .hero { margin-top: 18px; padding: 32px; border: 1px solid var(--stroke); border-radius: var(--radius); background: white; }
      .stack { margin-top: 18px; padding: 32px; border: 1px solid var(--stroke); border-radius: var(--radius); background: white; line-height: 1.6; }
      h2 { color: var(--accent); margin-top: 24px; }
      code { background: #f0f0f0; padding: 2px 4px; border-radius: 4px; }
    </style>
  </head>
  <body>
    <div class="shell">
      <div class="topbar">
        <div><strong>AGISystem2 Research</strong></div>
        <div class="top-links">
          <a class="pill" href="../index.html">Home</a>
          <a class="pill" href="../research.html">Back</a>
        </div>
      </div>
      <header class="hero">
        <h1>Democratization through Quantization</h1>
        <p>Technical analysis of local LLM inference via the llama.cpp ecosystem.</p>
      </header>
      <main class="stack">
        <h2><a href="https://github.com/ggml-org/llama.cpp" target="_blank">llama.cpp</a> and the GGML/GGUF Format</h2>
        <p>Developed by <a href="https://github.com/ggerganov" target="_blank">Georgi Gerganov</a>, <code>llama.cpp</code> is an implementation of Transformer inference in pure C/C++. The objective is to enable large model execution on commodity CPUs by eliminating the runtime overhead of high-level frameworks.</p>

        <h3>Aggressive <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)" target="_blank">Quantization</a> (k-quants)</h3>
        <p>The project focuses on mapping 16-bit floating-point weights to 4-bit and 5-bit integers. This reduction allows a 7B parameter model, which typically requires ~14GB VRAM, to operate within ~5GB of system RAM.</p>

        <h3>Hardware-Specific SIMD Optimization</h3>
        <p>Implementation details include manual optimization for <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions" target="_blank">AVX2/AVX-512</a> (x86) and <a href="https://en.wikipedia.org/wiki/ARM_architecture#Advanced_SIMD_(Neon)" target="_blank">NEON</a> (ARM). On Apple Silicon, the framework leverages high-bandwidth unified memory to achieve performance parity with entry-level discrete accelerators.</p>

        <h3>Ecosystem Integration</h3>
        <ul>
          <li><strong><a href="https://ollama.com/" target="_blank">Ollama</a>:</strong> A tool for packaging and serving local models through standardized APIs.</li>
          <li><strong><a href="https://lmstudio.ai/" target="_blank">LM Studio</a>:</strong> A graphical interface for the deployment and benchmarking of quantized models.</li>
        </ul>
      </main>
    </div>
  </body>
</html>