<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Theory - AGISystem2</title>
  <link rel="stylesheet" href="../reference/style.css">
</head>
<body>
  <div class="page">
  <div class="nav-header">
    <h1>Theoretical Foundation</h1>
    <small>
      <a href="../index.html">Home</a> &middot;
      <a href="../architecture/index.html">Architecture</a> &middot;
      <a href="index.html">Theory</a> &middot;
      <a href="../syntax/index.html">Syntax</a> &middot;
      <a href="../api/index.html">APIs</a> &middot;
      <a href="../wiki/index.html">Wiki</a> &middot;
      <a href="../specs/matrix.html">Specs</a>
    </small>
    <small>Hyperdimensional Computing for neuro-symbolic AI</small>
  </div>

  <div class="section-intro">
    <p>AGISystem2 is built on <strong>Hyperdimensional Computing (HDC)</strong>, a computational paradigm that represents information as high-dimensional vectors. This foundation enables deterministic, explainable reasoning with mathematical guarantees.</p>
  </div>

  <h2>Why Hyperdimensional Computing?</h2>

  <div class="section-grid">
    <div class="section-card">
      <h3>Determinism</h3>
      <p>Unlike probabilistic neural networks, HDC operations are fully deterministic. The same input always produces the same output, enabling perfect reproducibility and debugging.</p>
    </div>

    <div class="section-card">
      <h3>Compositionality</h3>
      <p>Complex structures can be built from simple parts using only two operations (Bind and Bundle). This enables systematic construction and deconstruction of knowledge.</p>
    </div>

    <div class="section-card">
      <h3>Noise Tolerance</h3>
      <p>High-dimensional representations are naturally robust to noise and errors. Small perturbations don't significantly affect similarity comparisons.</p>
    </div>

    <div class="section-card">
      <h3>Efficiency</h3>
      <p>Core operations are extremely fast on modern hardware. The mathematical structure allows for hardware-friendly implementations.</p>
    </div>
  </div>

  <h2>Core Concepts</h2>

  <h3>The Hypervector</h3>

  <p>The fundamental data structure is a <strong>hypervector</strong> - a high-dimensional representation where each concept occupies a unique region in the vector space.</p>

  <div class="alert alert-info">
    <strong>Key Insight:</strong> In high-dimensional spaces, randomly generated vectors are almost orthogonal to each other. This "quasi-orthogonality" property means any new concept gets a representation that doesn't interfere with existing ones.
  </div>

  <h3>The Three Core Operations</h3>

  <table>
    <tr>
      <th>Operation</th>
      <th>Purpose</th>
      <th>Mathematical Property</th>
    </tr>
    <tr>
      <td><strong>BIND</strong></td>
      <td>Create associations between concepts</td>
      <td>Self-inverse: (A BIND B) BIND B = A</td>
    </tr>
    <tr>
      <td><strong>BUNDLE</strong></td>
      <td>Combine multiple vectors into one</td>
      <td>Result is similar to all inputs</td>
    </tr>
    <tr>
      <td><strong>SIMILARITY</strong></td>
      <td>Measure relatedness</td>
      <td>Range [0, 1], 0.5 = unrelated</td>
    </tr>
  </table>

  <h3>Position Vectors</h3>

  <p>Because BIND is commutative, we need a mechanism to distinguish argument positions. Position vectors (Pos1, Pos2, ..., Pos20) solve this:</p>

  <pre><code>// Without positions: loves(John, Mary) = loves(Mary, John) (WRONG!)
// With positions:
fact = Loves BIND (Pos1 BIND John) BIND (Pos2 BIND Mary)  // loves(John, Mary)
fact = Loves BIND (Pos1 BIND Mary) BIND (Pos2 BIND John)  // loves(Mary, John) - DIFFERENT!</code></pre>

  <h3>The Reasoning Equation</h3>

  <p>All queries reduce to this fundamental principle:</p>

  <div class="alert alert-info">
    <strong>Answer = Knowledge BIND Query<sup>-1</sup></strong>
    <p>Since BIND is self-inverse, "unbinding" the known parts of a query from the knowledge base reveals the unknown parts.</p>
  </div>

  <h2>HDC Strategies</h2>

  <p>AGISystem2 implements multiple HDC strategies, each with different internal representations while maintaining the same mathematical contract:</p>

  <div class="section-grid">
    <div class="section-card">
      <h3>Dense-Binary</h3>
      <p>Classic HDC with fixed-length binary vectors. Uses XOR for binding and majority vote for bundling.</p>
      <p><a href="strategies/dense-binary.html">Full Documentation &rarr;</a></p>
    </div>

    <div class="section-card">
      <h3>Sparse Polynomial (SPHDC)</h3>
      <p>Set-based HDC with k integer exponents. Uses Cartesian XOR for binding and Jaccard index for similarity.</p>
      <p><a href="strategies/sparse-polynomial.html">Full Documentation &rarr;</a></p>
    </div>
  </div>

  <h2>Mathematical Guarantees</h2>

  <p>Regardless of which strategy is used, the following properties are guaranteed:</p>

  <ul>
    <li><strong>Closure:</strong> All operations produce valid hypervectors</li>
    <li><strong>Determinism:</strong> Same inputs always produce same outputs</li>
    <li><strong>Approximate Reversibility:</strong> BIND is self-inverse; BUNDLE is approximately recoverable</li>
    <li><strong>Compositionality:</strong> Complex structures are built from simple parts</li>
    <li><strong>Graceful Degradation:</strong> Accuracy decreases smoothly as capacity is reached</li>
  </ul>

  <h2>Holographic Representations</h2>

  <p>AGISystem2's approach to knowledge representation has deep roots in <strong>holographic computing</strong> - a paradigm where information is distributed across the entire representation rather than localized in specific positions. <a href="holographic-representations.html">Read the full documentation &rarr;</a></p>

  <h3>Historical Context</h3>

  <p>The concept of holographic representations in computing traces back to several foundational works:</p>

  <ul>
    <li><strong>1960s:</strong> Holography principles (Gabor) inspire distributed representation ideas</li>
    <li><strong>1990s:</strong> Plate's Holographic Reduced Representations (HRR) - circular convolution binding</li>
    <li><strong>2000s:</strong> Kanerva's Hyperdimensional Computing - binary vectors with XOR binding</li>
    <li><strong>2010s:</strong> Vector Symbolic Architectures (VSA) as unifying framework</li>
  </ul>

  <div class="alert alert-info">
    <strong>Key Insight:</strong> Just as in optical holography where each fragment contains information about the whole image, holographic computing distributes information across all dimensions of the vector. This provides natural noise tolerance and content-addressable memory.
  </div>

  <h3>The Holographic Property</h3>

  <p>When we bind two concepts A and B, the result contains "holographic traces" of both:</p>

  <pre><code>// A BIND B is "holographically" related to both A and B
// Unbinding recovers the original:
(A BIND B) BIND B = A    // B is the "key" to recover A
(A BIND B) BIND A = B    // A is the "key" to recover B</code></pre>

  <p>This is analogous to optical holography where the reference beam is needed to reconstruct the image.</p>

  <h3>A Programming Language for Holographic Computation</h3>

  <p>AGISystem2's DSL (Domain Specific Language) provides high-level primitives for holographic computation:</p>

  <table>
    <tr>
      <th>DSL Primitive</th>
      <th>Holographic Operation</th>
      <th>Purpose</th>
    </tr>
    <tr>
      <td><code>relation arg1 arg2</code></td>
      <td>Encode structured fact</td>
      <td>Create holographic record</td>
    </tr>
    <tr>
      <td><code>query relation ?var</code></td>
      <td>Holographic unbinding</td>
      <td>Content-addressable retrieval</td>
    </tr>
    <tr>
      <td><code>prove goal</code></td>
      <td>Recursive unbinding + matching</td>
      <td>Logical inference via retrieval</td>
    </tr>
    <tr>
      <td><code>bundle [facts]</code></td>
      <td>Superposition of records</td>
      <td>Build knowledge base</td>
    </tr>
  </table>

  <p>The DSL abstracts away the underlying vector operations, allowing programmers to think in terms of relations, rules, and queries while the system performs holographic computation underneath.</p>

  <h3>Backtracking and Search</h3>

  <p>The holographic representation enables interesting possibilities for search and backtracking:</p>

  <ul>
    <li><strong>Parallel Retrieval:</strong> All facts matching a query pattern are activated simultaneously</li>
    <li><strong>Confidence-Ordered Results:</strong> Multiple candidates emerge ranked by similarity</li>
    <li><strong>State Preservation:</strong> The vector state before binding can be preserved for backtracking</li>
    <li><strong>Compositional Rollback:</strong> Since BIND is self-inverse, computations can be "undone" algebraically</li>
  </ul>

  <h3>Connections to Homomorphic Encryption</h3>

  <p>There is an intriguing parallel between holographic representations and homomorphic encryption:</p>

  <div class="alert alert-warning">
    <strong>Conceptual Link:</strong> Just as homomorphic encryption allows computation on encrypted data without decryption, holographic binding allows <em>composition</em> of concepts without "unpacking" their internal structure. The vector for "loves(John, Mary)" is meaningful without explicitly storing "John" or "Mary" as separate retrievable entities.
  </div>

  <p>This suggests potential applications in:</p>

  <ul>
    <li><strong>Privacy-Preserving Reasoning:</strong> Queries can be answered without exposing raw facts</li>
    <li><strong>Secure Knowledge Sharing:</strong> Holographic representations can be shared without revealing atomic concepts (if concept vectors are kept private)</li>
    <li><strong>Deterministic Initialization:</strong> Using cryptographic hashes for concept vector generation provides both determinism and privacy</li>
  </ul>

  <p>While not equivalent to formal homomorphic encryption, the holographic paradigm offers similar "compute without decode" properties in a probabilistic, similarity-based setting.</p>

  <h2>Further Reading</h2>

  <ul>
    <li><a href="strategies/dense-binary.html">Dense-Binary Strategy - In-Depth Theory</a></li>
    <li><a href="strategies/sparse-polynomial.html">SPHDC Strategy - In-Depth Theory</a></li>
    <li><a href="../specsLoader.html?spec=DS01-Theoretical-Foundation.md">DS01 - Theoretical Foundation Specification</a></li>
    <li><a href="../specsLoader.html?spec=DS09-Core-HDC-Implementation.md">DS09 - Core HDC Implementation Specification</a></li>
    <li>Kanerva, P. (2009). "Hyperdimensional Computing: An Introduction to Computing in Distributed Representations"</li>
    <li>Plate, T. (2003). "Holographic Reduced Representation: Distributed Representation for Cognitive Structures"</li>
    <li>Gayler, R. (2003). "Vector Symbolic Architectures Answer Jackendoff's Challenges for Cognitive Neuroscience"</li>
  </ul>

  <div class="footer-nav">
    <p>HDC provides the mathematical foundation for deterministic, explainable AI with formal guarantees.</p>
  </div>
  </div>
</body>
</html>
