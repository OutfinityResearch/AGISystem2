<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Explainability - Trustworthy AI - AGISystem2</title>
  <link rel="stylesheet" href="../../reference/style.css">
  <style>
    .explanation-level {
      background: #f8f9fa;
      border-left: 4px solid #007bff;
      padding: 15px;
      margin: 15px 0;
    }
    .explanation-level h4 {
      margin-top: 0;
      color: #007bff;
    }
    .trace-box {
      background: #2d2d2d;
      color: #f8f8f2;
      padding: 15px;
      border-radius: 4px;
      overflow-x: auto;
      font-family: monospace;
      font-size: 13px;
    }
    .highlight-step { color: #50fa7b; }
    .highlight-rule { color: #ff79c6; }
    .highlight-fact { color: #f1fa8c; }
    .research-box {
      background: #e7f3ff;
      border: 1px solid #0066cc;
      border-left: 4px solid #0066cc;
      padding: 15px;
      margin: 15px 0;
    }
  </style>
</head>
<body>
  <div class="page">
  <div class="nav-header">
    <h1>Explainability in HDC Reasoning <span class="badge badge-warning">Research</span></h1>
    <small>
      <a href="../../index.html">Home</a> &middot;
      <a href="../index.html">Theory</a> &middot;
      <a href="index.html">Trustworthy AI</a> &middot;
      <a href="explainability.html">Explainability</a>
    </small>
    <small>Understanding how and why conclusions are reached</small>
  </div>

  <div class="section-intro">
    <p><strong>Explainability in AI means more than generating plausible-sounding text.</strong> AGISystem2 provides <em>actual proof traces</em>&mdash;the real computational steps that led to a conclusion, not a post-hoc confabulation.</p>
  </div>

  <h2>1. What Makes HDC Explainable?</h2>

  <p>Unlike neural networks where decisions emerge from billions of opaque weights, AGISystem2's reasoning is inherently transparent:</p>

  <div class="section-grid">
    <div class="section-card">
      <h3>Explicit Rules</h3>
      <p>Every inference corresponds to a named rule in the knowledge base. No implicit patterns learned from data.</p>
    </div>

    <div class="section-card">
      <h3>Traceable Steps</h3>
      <p>Each reasoning step is an explicit HDC operation (BIND, UNBIND, SIMILARITY). The proof trace IS the computation.</p>
    </div>

    <div class="section-card">
      <h3>Deterministic</h3>
      <p>Same input always produces same output. Explanations are reproducible and debuggable.</p>
    </div>

    <div class="section-card">
      <h3>Human-Readable KB</h3>
      <p>Knowledge is stored in DSL format readable by humans, not opaque weight matrices.</p>
    </div>
  </div>

  <h2>2. Anatomy of a Proof Trace</h2>

  <p>When AGISystem2 proves a goal, it generates a complete trace of reasoning steps:</p>

  <div class="trace-box">
<span class="highlight-step">GOAL:</span> prove(Animal(Spot))

<span class="highlight-step">STEP 1:</span> Query KB for direct fact Animal(Spot)
         Result: NOT FOUND (similarity 0.48)

<span class="highlight-step">STEP 2:</span> Search for applicable rules
         Found: <span class="highlight-rule">rule: if Dog(?x) then Animal(?x)</span>
         (Dog IS_A Animal - transitive inheritance)

<span class="highlight-step">STEP 3:</span> Unify rule with goal
         ?x = Spot
         New subgoal: Dog(Spot)

<span class="highlight-step">STEP 4:</span> Query KB for Dog(Spot)
         Result: <span class="highlight-fact">FOUND (similarity 0.97)</span>
         Source: explicit fact in knowledge base

<span class="highlight-step">STEP 5:</span> Apply rule [Dog IS_A Animal]
         Dog(Spot) ⟹ Animal(Spot)

<span class="highlight-step">CONCLUSION:</span> Animal(Spot) = TRUE
         Confidence: 0.97
         Proof depth: 2
         Rules applied: [Dog IS_A Animal]
  </div>

  <p>Every element of this trace is verifiable:</p>
  <ul>
    <li>The rule exists in the KB and can be inspected</li>
    <li>The fact Dog(Spot) can be verified</li>
    <li>The inference step follows logically from the rule</li>
  </ul>

  <h2>3. Multi-Level Explanations</h2>

  <p>Different stakeholders need different levels of detail:</p>

  <div class="explanation-level">
    <h4>Developer Level</h4>
    <p><strong>Audience:</strong> System developers, debuggers</p>
    <p><strong>Content:</strong> Full proof trace with vector operations, similarity scores, binding/unbinding steps</p>
    <p><strong>Example:</strong></p>
    <pre>prove(Animal(Spot)):
  match(Dog(Spot), KB) → sim=0.97
  apply_rule(Dog_IS_A_Animal, {?x=Spot})
  → Animal(Spot) [confidence=0.97, depth=2]</pre>
  </div>

  <div class="explanation-level">
    <h4>Domain Expert Level</h4>
    <p><strong>Audience:</strong> Domain specialists, knowledge engineers</p>
    <p><strong>Content:</strong> Key reasoning steps, rules applied, without HDC internals</p>
    <p><strong>Example:</strong> "Concluded Spot is an animal by applying the inheritance rule 'all dogs are animals' to the known fact that Spot is a dog."</p>
  </div>

  <div class="explanation-level">
    <h4>End User Level</h4>
    <p><strong>Audience:</strong> Non-technical users</p>
    <p><strong>Content:</strong> Natural language summary of the conclusion</p>
    <p><strong>Example:</strong> "Spot is an animal because Spot is a dog, and all dogs are animals."</p>
  </div>

  <div class="explanation-level">
    <h4>Auditor Level</h4>
    <p><strong>Audience:</strong> Compliance officers, legal reviewers</p>
    <p><strong>Content:</strong> Decision justification with rule citations</p>
    <p><strong>Example:</strong> "Decision: APPROVED. Basis: Rule KB.Ontology.001 (taxonomic inheritance) applied to established fact F.Dog.Spot."</p>
  </div>

  <h2>4. Contrastive Explanations</h2>

  <p>Often the most useful explanation answers "Why X instead of Y?"</p>

  <div class="trace-box">
<span class="highlight-step">QUESTION:</span> Why is Spot classified as Animal, not Plant?

<span class="highlight-step">ANALYSIS:</span>
  For Animal(Spot):
    - Dog(Spot) is TRUE [explicit fact]
    - Dog IS_A Animal [rule exists]
    ⟹ Animal(Spot) PROVABLE

  For Plant(Spot):
    - No direct fact Plant(Spot)
    - No rule Chain leading to Plant(Spot)
    - Dog IS_NOT_A Plant [explicit constraint]
    ⟹ Plant(Spot) NOT PROVABLE

<span class="highlight-step">CONTRASTIVE EXPLANATION:</span>
  "Spot is Animal (not Plant) because Spot is a Dog,
   dogs inherit from Animal (not Plant), and
   Dog and Plant are disjoint categories."
  </div>

  <h2>5. Counterfactual Explanations</h2>

  <p>Understanding what would change an outcome:</p>

  <div class="trace-box">
<span class="highlight-step">QUESTION:</span> What would make Animal(Spot) false?

<span class="highlight-step">COUNTERFACTUAL ANALYSIS:</span>
  Current proof depends on:
    1. Fact: Dog(Spot)
    2. Rule: Dog IS_A Animal

  To invalidate conclusion:
    Option A: Remove fact Dog(Spot)
    Option B: Remove rule Dog IS_A Animal
    Option C: Add exception: NOT Animal(Spot)

<span class="highlight-step">MINIMAL CHANGE:</span>
  "If Spot were not a dog, we could not conclude Spot is an animal
   (without additional information)."
  </div>

  <h2>6. Explanation Quality Metrics</h2>

  <p>AGISystem2 explanations can be evaluated on:</p>

  <table>
    <tr>
      <th>Metric</th>
      <th>Description</th>
      <th>How HDC Achieves It</th>
    </tr>
    <tr>
      <td><strong>Fidelity</strong></td>
      <td>Explanation matches actual reasoning</td>
      <td>Proof trace IS the computation</td>
    </tr>
    <tr>
      <td><strong>Completeness</strong></td>
      <td>All relevant steps included</td>
      <td>Full trace available, summarizable</td>
    </tr>
    <tr>
      <td><strong>Consistency</strong></td>
      <td>Same query → same explanation</td>
      <td>Deterministic operations</td>
    </tr>
    <tr>
      <td><strong>Verifiability</strong></td>
      <td>Claims can be checked</td>
      <td>Rules and facts inspectable</td>
    </tr>
    <tr>
      <td><strong>Minimality</strong></td>
      <td>No irrelevant details</td>
      <td>Proof traces are minimal paths</td>
    </tr>
  </table>

  <h2>7. Explainability and Bias Detection</h2>

  <p>Explainability enables systematic bias analysis:</p>

  <div class="research-box">
    <strong>Key Insight:</strong> Because we can see exactly which rules led to which conclusions, we can analyze whether certain rules disproportionately affect certain groups.
  </div>

  <ul>
    <li><strong>Rule impact analysis:</strong> Which rules most affect outcomes for group X vs Y?</li>
    <li><strong>Definition sensitivity:</strong> How do small changes in definitions change conclusions?</li>
    <li><strong>Path analysis:</strong> Do different demographic groups follow different reasoning paths?</li>
  </ul>

  <p>See <a href="bias-study.html">Bias Study</a> for detailed methodology.</p>

  <h2>8. Implementation Architecture</h2>

  <p>AGISystem2's explainability system:</p>

  <pre><code>
+--------------------------------------------------+
|              Explanation Generator               |
|    Transforms proof traces to target audience    |
+--------------------------------------------------+
|                Proof Trace Store                 |
|    Complete record of all reasoning steps        |
+--------------------------------------------------+
|               Reasoning Engine                   |
|    prove() / query() with trace logging          |
+--------------------------------------------------+
|                HDC Foundation                    |
|    Deterministic, traceable operations           |
+--------------------------------------------------+
  </code></pre>

  <h2>9. Research Directions</h2>

  <div class="research-box">
    <strong>Open Questions:</strong>
    <ul>
      <li>Automatic summarization of complex proof traces</li>
      <li>Natural language generation from formal traces</li>
      <li>Interactive explanation exploration ("why this rule?")</li>
      <li>Explanation personalization based on user expertise</li>
      <li>Visual representation of reasoning graphs</li>
    </ul>
  </div>

  <h2>Related Documentation</h2>

  <ul>
    <li><a href="index.html">Trustworthy AI Overview</a></li>
    <li><a href="bias-study.html">Bias Study Through Explainability</a></li>
    <li><a href="synthetic-data.html">Synthetic Data Generation</a></li>
    <li><a href="research.html">Research Directions</a></li>
    <li><a href="../index.html">Theory Overview</a></li>
  </ul>

  <div class="footer-nav">
    <p>
      <a href="index.html">&larr; Trustworthy AI Overview</a> |
      <a href="bias-study.html">Bias Study &rarr;</a>
    </p>
  </div>
  </div>
</body>
</html>
