<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Use Cases - AGISystem2</title>
  <link rel="stylesheet" href="../reference/style.css">
  <style>
    .tradeoff-note {
      background: #fffde7;
      border-left: 3px solid #ffc107;
      padding: 10px 15px;
      margin: 10px 0;
      font-size: 13px;
      color: #666;
    }
    .status-badge {
      display: inline-block;
      padding: 2px 8px;
      border-radius: 12px;
      font-size: 11px;
      font-weight: bold;
      margin-left: 8px;
    }
    .status-demonstrated { background: #c8e6c9; color: #2e7d32; }
    .status-implemented { background: #fff3e0; color: #e65100; }
    .status-theoretical { background: #e3f2fd; color: #1565c0; }
    .status-research { background: #e3f2fd; color: #1565c0; }
  </style>
</head>
<body>
  <div class="page">
  <div class="nav-header">
    <h1>AGISystem2 Use Cases <span class="badge badge-primary">Mixed</span></h1>
    <small>
      <a href="../index.html">Home</a> &middot;
      <a href="index.html">Use Cases</a>
    </small>
    <small>Practical applications with honest assessment</small>
  </div>

  <div class="section-intro">
    <p>AGISystem2 is designed for applications where <strong>formal verification</strong>, <strong>complete traceability</strong>, and <strong>explicit reasoning</strong> are requirements. This page describes realistic use cases, including their limitations and trade-offs.</p>
  </div>

  <div class="alert alert-warning" style="background: #fff3e0; border-left: 4px solid #ff9800; padding: 15px; margin: 20px 0; border-radius: 0 8px 8px 0;">
    <strong>Honest positioning:</strong> AGISystem2 complements LLMs for specific scenarios&mdash;it does not replace them. For many applications, LLMs alone are sufficient and more practical. Use AGISystem2 when the cost of errors or unexplainability is high.
  </div>

  <h2>When to Use AGISystem2</h2>

  <div class="section-grid" style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
    <div style="background: #e8f5e9; padding: 15px; border-radius: 8px;">
      <h4 style="margin-top: 0; color: #2e7d32;">Good Fit</h4>
      <ul style="margin-bottom: 0;">
        <li>Decisions must be formally explainable</li>
        <li>Regulatory compliance is mandatory</li>
        <li>Audit trails are legally required</li>
        <li>Domain can be formalized as rules</li>
        <li>Consistency checking is valuable</li>
        <li>Errors have significant consequences</li>
      </ul>
    </div>
    <div style="background: #ffebee; padding: 15px; border-radius: 8px;">
      <h4 style="margin-top: 0; color: #c62828;">Poor Fit</h4>
      <ul style="margin-bottom: 0;">
        <li>Open-ended creative tasks</li>
        <li>Ambiguous natural language input</li>
        <li>Domains that can't be formalized</li>
        <li>"Good enough" is acceptable</li>
        <li>Speed matters more than verification</li>
        <li>No resources for domain modeling</li>
      </ul>
    </div>
  </div>

  <h2>Capability Comparison (Nuanced)</h2>

  <table>
    <tr>
      <th>Capability</th>
      <th>LLMs</th>
      <th>AGISystem2</th>
      <th>Trade-off</th>
    </tr>
    <tr>
      <td>Pattern recognition</td>
      <td>Excellent</td>
      <td>Similarity-based only</td>
      <td>LLMs far superior for perception</td>
    </tr>
    <tr>
      <td>Multi-step reasoning</td>
      <td>Emergent, hard to verify</td>
      <td>Explicit formal chains</td>
      <td>AGISystem2 requires formalization</td>
    </tr>
    <tr>
      <td>Learning new facts</td>
      <td>Requires retraining/context</td>
      <td>Instant integration</td>
      <td>AGISystem2 doesn't generalize</td>
    </tr>
    <tr>
      <td>Explaining reasoning</td>
      <td>Post-hoc rationalization</td>
      <td>Actual derivation trace</td>
      <td>Traces can be technical</td>
    </tr>
    <tr>
      <td>Natural language</td>
      <td>Native capability</td>
      <td>Requires bridge layer</td>
      <td>AGISystem2 needs NL translation</td>
    </tr>
    <tr>
      <td>Handling ambiguity</td>
      <td>Robust</td>
      <td>Requires disambiguation</td>
      <td>LLMs more flexible</td>
    </tr>
  </table>

  <hr>

  <h2>Use Case 1: Compliance Verification <span class="status-badge status-research">Research</span></h2>

  <div class="alert alert-info" style="background: #e3f2fd; border-left: 4px solid #1976d2; padding: 15px; margin: 15px 0;">
    <strong>Value proposition:</strong> Encode regulations as formal rules. Check actions before execution. Generate proof traces (audit logging/export is external).
  </div>

  <div class="alert alert-warning" style="background: #fffde7; border-left: 4px solid #fbc02d; padding: 12px 15px; margin: 12px 0; border-radius: 0 8px 8px 0;">
    <strong>Status:</strong> Research pattern (not shipped as runnable Core/config). Examples are illustrative; audit logging/export requires external integration.
  </div>

  <h3>The Scenario</h3>
  <p>Organizations face regulatory requirements (GDPR, HIPAA, internal policies) where violations have significant consequences. Current approach: periodic audits that find problems after they've occurred.</p>

  <h3>How AGISystem2 Helps</h3>
  <ul>
    <li><strong>Formal rule encoding:</strong> Regulations expressed as checkable constraints</li>
    <li><strong>Pre-action validation:</strong> Operations checked before execution</li>
    <li><strong>Proof traces:</strong> Every decision traceable to specific rules (audit logging/export is external)</li>
    <li><strong>Remediation guidance:</strong> Not just "violation" but "here's how to fix it"</li>
  </ul>

  <h3>Example</h3>
  <pre><code>Action: MarketingTeam processes CustomerEmails
Purpose: NewProductCampaign

GDPR Analysis:
  ✗ art6_consent: NOT SATISFIED
    Existing consent: "marketing communications" (2023-06-01)
    Required: Consent specific to NewProductCampaign

Required remediation:
  Option A: Obtain specific consent
  Option B: Complete Legitimate Interest Assessment</code></pre>

  <div class="tradeoff-note">
    <strong>Trade-off:</strong> Requires upfront investment in formalizing regulations. Ambiguous regulatory language must be interpreted and encoded&mdash;this interpretation is a human judgment, not automatic. The system verifies against the formalization, not against "true" regulatory intent.
  </div>

  <p><a href="../theory/trustworthy-ai/compliance.html">Compliance documentation &rarr;</a></p>

  <hr>

  <h2>Use Case 2: Agent Plan Verification <span class="status-badge status-research">Research</span></h2>

  <div class="alert alert-info" style="background: #e3f2fd; border-left: 4px solid #1976d2; padding: 15px; margin: 15px 0;">
    <strong>Value proposition:</strong> Verify AI agent plans before execution. Catch invalid plans before they cause damage.
  </div>

  <div class="alert alert-warning" style="background: #fffde7; border-left: 4px solid #fbc02d; padding: 12px 15px; margin: 12px 0; border-radius: 0 8px 8px 0;">
    <strong>Status:</strong> Research pattern (not shipped as runnable Core/config). Plan generation/monitoring requires external integration; AGISystem2 can model and validate semantics.
  </div>

  <h3>The Scenario</h3>
  <p>LLM-based agents generate action plans, but these plans may violate preconditions or have unintended effects. Failures only become apparent at execution time.</p>

  <h3>How AGISystem2 Helps</h3>
  <ul>
    <li><strong>Formal tool semantics:</strong> Each tool's preconditions and effects defined</li>
    <li><strong>Pre-execution checking:</strong> Plans validated before any action runs</li>
    <li><strong>Failure diagnosis:</strong> Missing preconditions explicitly identified</li>
    <li><strong>Alternative generation:</strong> Valid alternatives suggested when plans fail</li>
  </ul>

  <h3>Example</h3>
  <pre><code>Goal: "Send Q3 report to team"

Generated plan:
  1. Login(SalesDB)        ✓ Pre: HasCredentials - satisfied
  2. QueryDatabase(Q3)     ✓ Pre: Connected - will be satisfied
  3. Login(EmailServer)    ✓ Pre: HasCredentials - satisfied
  4. SendEmail(Report)     ✓ Pre: HasData - will be satisfied

Plan Status: VALID ✓</code></pre>

  <div class="tradeoff-note">
    <strong>Trade-off:</strong> Every tool must have formally defined preconditions and effects. This works for well-defined APIs but is difficult for fuzzy real-world actions. Plan generation itself still relies on LLMs; AGISystem2 only verifies.
  </div>

  <p><a href="../theory/trustworthy-ai/agent-planning.html">Agent planning documentation &rarr;</a></p>

  <hr>

  <h2>Use Case 3: Knowledge Base Reasoning <span class="status-badge status-demonstrated">Demonstrated</span></h2>

  <div class="alert alert-info" style="background: #e3f2fd; border-left: 4px solid #1976d2; padding: 15px; margin: 15px 0;">
    <strong>Value proposition:</strong> Taxonomic inference with complete traceability. Every conclusion traceable to source facts and rules.
  </div>

  <h3>The Scenario</h3>
  <p>Organizations need to query knowledge bases and understand exactly how answers were derived. "Why is X true?" should have a verifiable answer.</p>

  <h3>How AGISystem2 Helps</h3>
  <ul>
    <li><strong>Transitive inference:</strong> Automatic reasoning through IS_A hierarchies</li>
    <li><strong>Proof traces:</strong> Complete derivation available for every query</li>
    <li><strong>Contradiction detection:</strong> Inconsistencies flagged explicitly</li>
    <li><strong>Incremental updates:</strong> New facts integrate without retraining</li>
  </ul>

  <h3>Example</h3>
  <pre><code>Facts:
  isA(Spot, Dog)
  isA(Dog, Animal)
  isA(Animal, LivingThing)

Query: isA(Spot, LivingThing)?

Result: TRUE
Proof:
  1. isA(Spot, Dog) [direct fact]
  2. isA(Dog, Animal) [direct fact]
  3. isA(Animal, LivingThing) [direct fact]
  4. isA(Spot, LivingThing) [transitive chain: 1→2→3]</code></pre>

  <div class="tradeoff-note">
    <strong>Trade-off:</strong> Works well for taxonomies and rule-based domains. Does not handle probabilistic reasoning, fuzzy categories, or exceptions gracefully. Knowledge must be explicitly encoded&mdash;the system doesn't learn from examples.
  </div>

  <hr>

  <h2>Use Case 4: Thinking Database (RAG Alternative) <span class="status-badge status-implemented">Implemented</span></h2>

  <div class="alert alert-info" style="background: #e3f2fd; border-left: 4px solid #1976d2; padding: 15px; margin: 15px 0;">
    <strong>Value proposition:</strong> Replace semantic search approximations with verifiable logical reasoning.
    Get real answers derived from formal proofs, not statistical similarity matches.
  </div>

  <h3>The Problem with RAG</h3>
  <p>Retrieval-Augmented Generation (RAG) finds "similar" documents using vector embeddings, but:</p>
  <ul>
    <li><strong>Similarity ≠ Relevance:</strong> Semantically similar text may not contain the actual answer</li>
    <li><strong>No reasoning:</strong> RAG retrieves, it doesn't think&mdash;multi-step inference is impossible</li>
    <li><strong>Hallucination risk:</strong> LLM may still hallucinate even with retrieved context</li>
    <li><strong>No provenance:</strong> "Where did this answer come from?" is hard to answer</li>
  </ul>

  <h3>The Thinking Database Approach</h3>
  <p>AGISystem2 as a "thinking database" provides:</p>
  <ul>
    <li><strong>Logical derivation:</strong> Answers are <em>proved</em>, not retrieved</li>
    <li><strong>Multi-step reasoning:</strong> Chains facts and rules to reach conclusions</li>
    <li><strong>Formal proof trace:</strong> Every answer has a derivation trace</li>
    <li><strong>Consistency guarantees:</strong> Contradictions are detected, not hidden</li>
  </ul>

  <h3>Comparison</h3>
  <table>
    <tr>
      <th>Aspect</th>
      <th>RAG (Semantic Search)</th>
      <th>Thinking Database</th>
    </tr>
    <tr>
      <td>Query</td>
      <td>"Find similar documents"</td>
      <td>"Prove this is true"</td>
    </tr>
    <tr>
      <td>Answer source</td>
      <td>Retrieved text chunks</td>
      <td>Logical derivation</td>
    </tr>
    <tr>
      <td>Multi-step reasoning</td>
      <td>No (single retrieval)</td>
      <td>Yes (chained inference)</td>
    </tr>
    <tr>
      <td>Explainability</td>
      <td>"These docs were similar"</td>
      <td>"Here's the proof"</td>
    </tr>
    <tr>
      <td>Consistency</td>
      <td>Can return contradictions</td>
      <td>Contradictions detected</td>
    </tr>
  </table>

  <h3>Example</h3>
  <pre><code>Question: "Can employees in the Finance department access customer data?"

<strong>RAG approach:</strong>
  → Vector search finds: "Finance handles sensitive financial data..."
  → LLM generates: "Yes, Finance can access data" (WRONG - no reasoning)

<strong>Thinking Database approach:</strong>
  Knowledge Base:
    - Finance department handles FinancialData
    - CustomerData requires CustomerDataAccess permission
    - Finance department has FinancialDataAccess permission
    - CustomerDataAccess ≠ FinancialDataAccess

  Query: canAccess(Finance, CustomerData)?

  Result: FALSE
  Proof:
    1. canAccess(X, CustomerData) requires hasPermission(X, CustomerDataAccess)
    2. hasPermission(Finance, FinancialDataAccess) [fact]
    3. NOT hasPermission(Finance, CustomerDataAccess) [no matching fact]
    4. Therefore: canAccess(Finance, CustomerData) = FALSE</code></pre>

  <div class="tradeoff-note">
    <strong>Trade-off:</strong> Requires structured knowledge base (not free-form documents). Best for domains with clear rules and relationships. Can be combined with RAG: use RAG for initial retrieval, then verify with formal reasoning.
  </div>

  <hr>

  <h2>Use Case 5: LLM Output Verification <span class="status-badge status-theoretical">Theoretical</span></h2>

  <div class="alert alert-info" style="background: #e3f2fd; border-left: 4px solid #1976d2; padding: 15px; margin: 15px 0;">
    <strong>Value proposition:</strong> Check LLM outputs against formal constraints. Catch inconsistencies and hallucinations.
  </div>

  <h3>The Scenario</h3>
  <p>LLMs generate content that may contain factual errors or violate domain constraints. Human review is expensive and inconsistent.</p>

  <h3>How AGISystem2 Could Help</h3>
  <ul>
    <li><strong>Constraint checking:</strong> Verify LLM outputs against known rules</li>
    <li><strong>Consistency checking:</strong> Flag contradictions with established facts</li>
    <li><strong>Confidence calibration:</strong> Distinguish verified from unverified claims</li>
  </ul>

  <h3>Conceptual Example</h3>
  <pre><code>LLM output: "The water boiled at 50°C at standard pressure"

Knowledge base constraint:
  boilingPoint(Water, StandardPressure) = 100°C

Verification result: INCONSISTENT
  LLM claimed: 50°C
  Known value: 100°C
  Recommendation: Flag for review or reject</code></pre>

  <div class="tradeoff-note">
    <strong>Trade-off:</strong> Only catches errors that contradict formalized knowledge. Novel hallucinations outside the knowledge base cannot be detected. Requires extracting structured claims from LLM text (itself error-prone). This is a research direction, not a deployed capability.
  </div>

  <hr>

  <h2>Use Case 6: Scientific Reasoning Support <span class="status-badge status-theoretical">Theoretical</span></h2>

  <div class="alert alert-info" style="background: #e3f2fd; border-left: 4px solid #1976d2; padding: 15px; margin: 15px 0;">
    <strong>Value proposition:</strong> Encode theories formally. Check claims against established knowledge. Explore hypotheticals.
  </div>

  <h3>The Scenario</h3>
  <p>Scientific claims need validation against established theory. "What if?" hypotheses are difficult to explore systematically.</p>

  <h3>How AGISystem2 Could Help</h3>
  <ul>
    <li><strong>Theory encoding:</strong> Laws and constraints as formal structures</li>
    <li><strong>Claim validation:</strong> New results checked for consistency</li>
    <li><strong>Hypothesis exploration:</strong> "What would follow if X?"</li>
  </ul>

  <h3>Conceptual Example</h3>
  <pre><code>Claim: "Our engine achieved 95% efficiency"
       Operating between 300K and 400K

Theoretical check:
  Carnot limit = 1 - (300/400) = 25%

Result: INCONSISTENT WITH THERMODYNAMICS
  Claimed: 95%
  Maximum possible: 25%</code></pre>

  <div class="tradeoff-note">
    <strong>Trade-off:</strong> Requires formalizing scientific theories&mdash;a significant undertaking. Works for well-understood, rule-based domains (thermodynamics limits, conservation laws). Does not handle cutting-edge research where the rules themselves are uncertain. This is a research direction.
  </div>

  <hr>

  <h2>Implementation Reality</h2>

  <h3>What's Actually Working Today</h3>
  <ul>
    <li><strong>Taxonomic inference:</strong> IS_A hierarchies with transitive reasoning (126/126 eval tests passing)</li>
    <li><strong>Rule-based reasoning:</strong> Backward chaining with And/Or conditions</li>
    <li><strong>Proof traces:</strong> Complete derivation history for every conclusion</li>
    <li><strong>Basic NL output:</strong> DSL results converted to readable text</li>
  </ul>

  <h3>What's In Development</h3>
  <ul>
    <li>NL→DSL translation (requires LLM bridge)</li>
    <li>Trustworthy AI pattern library</li>
    <li>Integration examples with real systems</li>
  </ul>

  <h3>What's Theoretical</h3>
  <ul>
    <li>Large-scale deployment validation</li>
    <li>Complex domain formalization methodologies</li>
    <li>Hybrid LLM+AGISystem2 architectures</li>
  </ul>

  <h2>Getting Started (Realistic)</h2>

  <ol>
    <li><strong>Assess fit:</strong> Does your use case require formal verification? Is the domain formalizable?</li>
    <li><strong>Scope narrowly:</strong> Start with a specific, well-defined sub-domain</li>
    <li><strong>Formalize carefully:</strong> Encode domain knowledge as theories (this is the hard part)</li>
    <li><strong>Validate incrementally:</strong> Test with known cases before deploying</li>
    <li><strong>Maintain continuously:</strong> Theories need updating as requirements change</li>
  </ol>

  <h2>Related Documentation</h2>

  <ul>
    <li><a href="../index.html">Main Documentation</a> &ndash; overview and positioning</li>
    <li><a href="../theory/trustworthy-ai/index.html">Trustworthy AI Overview</a></li>
    <li><a href="../theory/grounding-problem.html">Symbol Grounding Problem</a> &ndash; epistemological limitations</li>
    <li><a href="../api/index.html">API Documentation</a></li>
  </ul>

  <div class="footer-nav">
    <p>
      <a href="../index.html">&larr; Documentation Home</a>
    </p>
  </div>
  </div>
</body>
</html>
