<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Data Ingestion</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="page">
  <div class="nav-header">
    <h1>How Data Enters</h1>
    <small><a href="index.html">Back to index</a> · <a href="quick_wiki.html">Quick wiki</a></small>
  </div>
  <p>The ingestion pipeline is how raw text becomes geometry in AGISystem2. It is intentionally simple and linear so that every step can be inspected and replayed. A sentence arrives, is rewritten into the constrained grammar, is parsed into a small tree, and is then encoded into a high-dimensional vector. That vector shapes bounded diamonds in ConceptStore and is indexed for later retrieval. This chapter follows that conveyor from left to right and points to the relevant module specifications and tests.</p>

  <h2>From Text to Normalised Sentences</h2>
  <p>In many applications, incoming text will be messy: natural language questions, documentation excerpts, logs, or transcripts. Before any of this can influence the conceptual space, TranslatorBridge must turn it into the small dialect described in the Grammar chapter. Given an input paragraph, the bridge uses a pinned language model and prompt to produce one or more canonical sentences such as <code>Dog IS_A Animal</code> or <code>ExportData PROHIBITED_BY GDPR</code>. Its behaviour is deterministic for a given configuration, and it reports its model and prompt versions so that later you can repeat the same normalisation if needed.</p>
  <p>If the bridge cannot find a safe rewrite into subject–relation–object form, it should decline the input rather than guess. This rule applies especially in safety-critical contexts, where a mis-normalised sentence could warp the conceptual space in hard-to-detect ways. The test suite under <code>.specs/tests/translator_bridge_normalization</code> contains examples of both successful and intentionally failing normalisations, ensuring that surprising phrases are either handled explicitly or rejected.</p>

  <h2>Parsing and Encoding</h2>
  <p>Once normalised sentences are available, the Parser builds a shallow tree for each: a root node for the subject, a relation node, and one or more child nodes for objects or properties. The recursion horizon limits how deep this tree may become; tokens beyond that depth are either ignored or treated neutrally to prevent runaway complexity. The encoder then walks this tree. For every edge from parent to child, it applies a relation-specific permutation to the child’s vector and adds it to the parent using saturated int8 arithmetic implemented by MathEngine.</p>
  <p>This process yields a single high-dimensional vector per sentence that encodes both the identities of the tokens and the roles they played. Because the same relations always use the same permutations and all arithmetic is deterministic, two identical sentences ingested under the same configuration will always produce identical vectors. The specifications in <code>.specs/ingest/parser.js.md</code> and <code>.specs/ingest/encoder.js.md</code> provide more technical detail, and the <code>.specs/tests/core_math</code> and <code>.specs/tests/relation_permuter</code> suites verify that permutations and additions behave as expected.</p>

  <h2>Clustering into Concepts</h2>
  <p>Each encoded sentence is both a new fact and a clue about the shape of the concept it refers to. The ClusterManager examines the new vector relative to existing diamonds for the target concept. If the point falls comfortably inside one of them, the corresponding diamond may be tightened or slightly adjusted. If it lies outside all current diamonds but close enough to be considered a plausible extension, a diamond may be widened. If it sits far from existing regions, ClusterManager may decide to create a new diamond, representing a distinct sense or context.</p>
  <p>This clustering logic turns a stream of discrete observations into evolving, continuous regions in conceptual space. It supports both conservative updates, where concepts change slowly as more evidence accumulates, and more dramatic restructurings, such as splitting a single overloaded concept into several more specific ones. The specifications in <code>.specs/ingest/clustering.js.md</code> and the tests in <code>.specs/tests/dimensions_catalog</code> and related suites demonstrate how these decisions are made and how they preserve determinism.</p>

  <h2>Persistence and Indexing</h2>
  <p>After clustering decides how to adjust concepts, ConceptStore persists the resulting diamonds using the Storage modules described in <code>.specs/support/storage.js.md</code>. In test profiles this may mean writing compact binary blobs to a temporary directory; in production it means persisting under a chosen root with optional custom backends. Either way, the goal is that reloading a store reproduces the same conceptual space, down to min/max bounds, centres, radii, and relevance masks.</p>
  <p>At the same time, Retriever updates its index so that subsequent queries can find relevant concepts quickly. It feeds the new vectors into the locality-sensitive hashing scheme configured for the current profile, computing hash values, band keys, and bucket assignments. Because hashing is seed-based and index parameters are fixed per profile, repeated ingestion of the same data set in the same configuration yields identical index structures. Tests under <code>.specs/tests/relation_permuter</code>, <code>.specs/tests/reason_smoke</code>, and <code>.specs/tests/persistence_file_binary</code> check that persisted and reloaded stores lead to the same retrieval behaviour.</p>

  <h2>End-to-End Example</h2>
  <p>Consider the sentence "Water HAS_PROPERTY boiling_point=100". TranslatorBridge either emits this form directly or normalises a more natural sentence into it. Parser and encoder then compute a vector where dimensions related to temperature, physicality, and process dynamics carry most of the signal. ClusterManager notices that this point strengthens the existing "Water" concept’s association with a particular temperature range and adjusts the corresponding diamond accordingly. ConceptStore writes the new diamond to disk, and Retriever records the new vector in its index. A subsequent question "Is water boiling at 100?" will follow the query path described in the Reasoning and API chapters and, if configurations align, land squarely inside the updated region.</p>
  <p>Throughout this process, provenance links every change back to the original text, the normalisation steps, the profile in use, and the seeds that shaped hashing and permutations. The Ingestion story therefore fits into the larger explainability and bias-control picture: data does not simply disappear into weights, but remains traceable as points and regions in a well-defined conceptual space.</p>

  <div class="footer-nav">
    <a href="index.html">Back to index</a>
    <a href="quick_wiki.html">Quick wiki</a>
  </div>
  </div>
</body>
</html>
