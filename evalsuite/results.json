{
  "timestamp": "2025-12-01T21:54:21.092Z",
  "totalPassed": 248,
  "totalFailed": 0,
  "cases": [
    {
      "id": "suite_01_ontology",
      "name": "Comprehensive Ontology & Taxonomy",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Is Fido a living thing?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Fido is a living thing through the chain: Fido → dog → mammal → animal → living thing.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.81,\"method\":\"transitive\",\"depth\":4,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"living_thing\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 3/8 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Is Sparky a mammal?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Sparky is a bird and birds are disjoint with mammals.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_path\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"mammal\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Is Whiskers an animal?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Whiskers is a cat, cats are mammals, mammals are animals.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.86,\"method\":\"transitive\",\"depth\":3,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"animal\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Is Paris in Europe?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Paris is in France and France is in Europe.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.9,\"method\":\"transitive\",\"depth\":2,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Paris\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Is Tokyo in Europe?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Tokyo is in Japan which is in Asia, not Europe.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_path\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Tokyo\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 1/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Is a software engineer an engineer?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, software engineers are a type of engineer.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"engineer\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Do doctors help patients?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, doctors are medical professionals and medical professionals help patients.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"inheritance\",\"inheritedFrom\":\"medical_professional\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"doctor\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 1 (-), 2/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Does a Tesla have wheels?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Tesla is a car and cars have wheels.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"inheritance\",\"inheritedFrom\":\"car\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Tesla\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 1 (-), 1/2 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_02_causation",
      "name": "Comprehensive Causation & Abduction",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Does infection cause fever?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, infection causes fever.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"infection\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Does friction cause expansion?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, friction causes heat and heat causes expansion.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.9,\"method\":\"transitive\",\"depth\":2,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"friction\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "What could cause John's symptoms?",
          "expectedTruth": "PLAUSIBLE",
          "expectedNatural": "John has fever and coughing, so respiratory infection is most likely.",
          "actualResponse": "{\"hypothesis\":\"infection\",\"band\":\"PLAUSIBLE\",\"hypotheses\":[{\"hypothesis\":\"infection\",\"band\":\"PLAUSIBLE\",\"priorityScore\":0.5,\"depth\":0,\"isTransitive\":false,\"combinedScore\":0.5,\"viaFact\":{\"subject\":\"infection\",\"relation\":\"CAUSES\",\"object\":\"fever\"}},{\"hypothesis\":\"inflammation\",\"band\":\"PLAUSIBLE\",\"priorityScore\":0.46,\"depth\":0,\"isTransitive\":false,\"combinedScore\":0.46,\"viaFact\":{\"subject\":\"inflammation\",\"relation\":\"CAUSES\",\"object\":\"fever\"}}]}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong PLAUSIBLE indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Could Mary have food poisoning?",
          "expectedTruth": "PLAUSIBLE",
          "expectedNatural": "Yes, Mary has stomach pain and nausea which are caused by food poisoning.",
          "actualResponse": "{\"hypothesis\":\"food_poisoning\",\"band\":\"PLAUSIBLE\",\"hypotheses\":[{\"hypothesis\":\"food_poisoning\",\"band\":\"PLAUSIBLE\",\"priorityScore\":0.52,\"depth\":0,\"isTransitive\":false,\"combinedScore\":0.52,\"viaFact\":{\"subject\":\"food_poisoning\",\"relation\":\"CAUSES\",\"object\":\"stomach_pain\"}}]}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong PLAUSIBLE indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Does deforestation cause extreme weather?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, deforestation causes climate change which causes extreme weather.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.9,\"method\":\"transitive\",\"depth\":2,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"deforestation\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Can electricity cause magnetism?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, electricity causes magnetism.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"electricity\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Does John have influenza?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "Not certain - John has fever and coughing but we don't know about fatigue.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_evidence\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"John\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "What causes headaches?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Headaches can be caused by dehydration, stress, or high blood pressure.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"dehydration\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/6 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_03_deontic",
      "name": "Comprehensive Deontic Rules & Permissions",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Can a medical drone fly over a hospital?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, medical drones are permitted to fly over hospitals.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"medical_drone\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Is a commercial drone allowed in the city center?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, commercial drones are prohibited from city centers.",
          "actualResponse": "{\"truth\":\"FALSE\",\"confidence\":1,\"method\":\"prohibited\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"commercial_drone\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 2/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Can an intern access confidential files?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, interns are prohibited from accessing confidential files.",
          "actualResponse": "{\"truth\":\"FALSE\",\"confidence\":1,\"method\":\"prohibited\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"intern\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Can a manager access confidential files?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, managers are permitted to access confidential files.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"manager\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Does an international transfer require a compliance check?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, international transfers require a compliance check.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"international_transfer\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Can a nurse prescribe medication?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, nurses can only administer medication, not prescribe it. Only doctors can prescribe.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_evidence\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"nurse\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 0/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Is patient consent required for treatment?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, treatment requires patient consent.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"treatment\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Can a customer view their own account?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, customers are permitted to view their own accounts.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"customer\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_04_counterfactual",
      "name": "Comprehensive Counterfactual & Temporal",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "If water boiled at 50 degrees, would it still be liquid at room temperature?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, if water boiled at 50 degrees and room temperature is 20 degrees, water would still be liquid since 20 < 50.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"celsius_20\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "What if there was no oxygen? Could fire exist?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, fire requires oxygen to burn. Without oxygen, fire cannot exist.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"fire\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/7 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Did World War 1 happen before World War 2?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, World War 1 happened before World War 2.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"world_war_1\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 2/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Did smartphones exist before personal computers?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, smartphones came after personal computers.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_path\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"smartphone\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Can you manufacture before designing?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, design must come before manufacturing.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_path\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"manufacturing\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Weak NL Match (DSL UNKNOWN but 1 text indicators for FALSE)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Is approval needed before implementation?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, approval is required before implementation.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"approval\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "If humans had no water, could they survive?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, humans require water to survive.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"human\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Does testing come before or after development?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Testing comes after development.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"testing\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/4 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_05_bias_masking",
      "name": "Comprehensive Bias Control & Masking",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Who should be selected for the role requiring PMP?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Alice or Carol should be selected as they have the required PMP certification. Bob does not qualify despite more experience.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Alice\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Does Bob qualify for the role?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Bob lacks the required PMP certification.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_evidence\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Bob\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Should Dan and Eve have the same pay?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, both are level 2 engineers with good performance, so pay should be equal.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"level_2_engineer\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Is it factual that the city council approved the budget?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, the city council approved the budget on Monday. This is a factual claim independent of political framing.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"city_council\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/9 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Did Team Delta deploy v2?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Team Delta deployed v2. This fact stands regardless of any emotional language.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"Team_Delta\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Should gender affect hiring decisions?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, gender is not a factor in hiring decisions.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"gender\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Does Alice meet the certification requirement?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Alice has the required PMP certification.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Alice\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 1 (-), 1/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Are Dan and Eve at the same level?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, both Dan and Eve are level 2 engineers.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"level_2_engineer\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 1 (-), 1/2 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_06_analogic_symmetric",
      "name": "Analogical Reasoning & Symmetric/Inverse Relations",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Is Ion married to Maria?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Ion is married to Maria. This is inferred from the symmetric property of MARRIED_TO - since Maria is married to Ion, Ion is also married to Maria.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"symmetric\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Ion\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: symmetric)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Is Ana a child of Maria?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Ana is the child of Maria. This is inferred from the inverse relation - since Maria is parent of Ana, Ana is child of Maria.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"inverse\",\"inverseRelation\":\"PARENT_OF\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Ana\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 1 (-), 3/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Is Mihai a sibling of Ana?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Mihai is a sibling of Ana. SIBLING_OF is symmetric, so if Ana is sibling of Mihai, then Mihai is sibling of Ana.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"symmetric\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"Mihai\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 2/8 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Bucharest is to Romania as Paris is to what?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "France. The analogy is: Bucharest is the capital of Romania, so Paris (being a capital) is the capital of France.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"analogy\":\"Bucharest : Romania :: Paris : Mihai\",\"result\":\"Mihai\",\"delta\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-24,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,37,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,57,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,34,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,48,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-31,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-55,0,0,0,0,0,0,0,0,0,0,0,0,51,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-42,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-23,27,0,0,0],\"confidence\":0.9835968017578125}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 3/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Doctor is to patient as teacher is to what?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Student. The analogy is: Doctor treats patient, so teacher (in the same professional pattern) teaches student.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"analogy\":\"doctor : patient :: teacher : Mihai\",\"result\":\"Mihai\",\"delta\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-30,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,50,0,0,0,-44,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,34,0,0,0,0,0,0,0,0,0,0,0,49,0,0,0,0,0,33,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-21,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-26,0,0,0,0,0,0,-34,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-29,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-58,0,0,0,0,0,0,42,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-48,0,0,0,0,0,0,0,0],\"confidence\":0.9819488525390625}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 3/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Find all capital-country pairs",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Found 6 pairs: Bucharest-Romania, Paris-France, Berlin-Germany, Madrid-Spain, Rome-Italy, London-United Kingdom.",
          "actualResponse": "[{\"subject\":\"Bucharest\",\"relation\":\"CAPITAL_OF\",\"object\":\"Romania\"},{\"subject\":\"Paris\",\"relation\":\"CAPITAL_OF\",\"object\":\"France\"},{\"subject\":\"Berlin\",\"relation\":\"CAPITAL_OF\",\"object\":\"Germany\"},{\"subject\":\"Madrid\",\"relation\":\"CAPITAL_OF\",\"object\":\"Spain\"},{\"subject\":\"Rome\",\"relation\":\"CAPITAL_OF\",\"object\":\"Italy\"},{\"subject\":\"London\",\"relation\":\"CAPITAL_OF\",\"object\":\"United_Kingdom\"}]",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/9 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Who are the children of Ion?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Ion has two children: Ana and Mihai.",
          "actualResponse": "[{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Ana\"},{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Mihai\"}]",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/2 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Who are Ana's parents?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Ana's parents are Maria and Ion.",
          "actualResponse": "[{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Ana\"},{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Ana\"}]",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/3 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_07_compositional_rules",
      "name": "Compositional Rules & Forward Chaining",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Is Ion the grandparent of Andrei?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Ion is the grandparent of Andrei. Ion is parent of Maria, and Maria is parent of Andrei, so by the grandparent rule, Ion is grandparent of Andrei.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"method\":\"composition\",\"rule\":\"grandparent_rule\",\"confidence\":0.9,\"proof\":{\"goal\":{\"subject\":\"Ion\",\"relation\":\"GRANDPARENT_OF\",\"object\":\"Andrei\"},\"steps\":[{\"pattern\":{\"subject\":\"?x\",\"relation\":\"PARENT_OF\",\"object\":\"?y\"},\"match\":{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Maria\"}},{\"pattern\":{\"subject\":\"?y\",\"relation\":\"PARENT_OF\",\"object\":\"?z\"},\"match\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"proof\":{\"goal\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"steps\":[{\"fact\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"justification\":\"direct_match\"}],\"valid\":true}}],\"rule\":\"grandparent_rule\",\"valid\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Ion\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 1 (-), 6/11 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Is Ana the grandparent of Elena?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Ana is the grandparent of Elena. Ana is parent of Maria, and Maria is parent of Elena.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"method\":\"composition\",\"rule\":\"grandparent_rule\",\"confidence\":0.9,\"proof\":{\"goal\":{\"subject\":\"Ana\",\"relation\":\"GRANDPARENT_OF\",\"object\":\"Elena\"},\"steps\":[{\"pattern\":{\"subject\":\"?x\",\"relation\":\"PARENT_OF\",\"object\":\"?y\"},\"match\":{\"subject\":\"Ana\",\"relation\":\"PARENT_OF\",\"object\":\"Maria\"}},{\"pattern\":{\"subject\":\"?y\",\"relation\":\"PARENT_OF\",\"object\":\"?z\"},\"match\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Elena\"},\"proof\":{\"goal\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Elena\"},\"steps\":[{\"fact\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Elena\"},\"justification\":\"direct_match\"}],\"valid\":true}}],\"rule\":\"grandparent_rule\",\"valid\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Ana\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 1 (-), 4/7 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Is Vasile an uncle of Maria?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Vasile is an uncle of Maria. Vasile is sibling of Ion, and Ion is parent of Maria.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"method\":\"composition\",\"rule\":\"uncle_aunt_rule\",\"confidence\":0.9,\"proof\":{\"goal\":{\"subject\":\"Vasile\",\"relation\":\"UNCLE_OR_AUNT_OF\",\"object\":\"Maria\"},\"steps\":[{\"pattern\":{\"subject\":\"?x\",\"relation\":\"SIBLING_OF\",\"object\":\"?y\"},\"match\":{\"subject\":\"Vasile\",\"relation\":\"SIBLING_OF\",\"object\":\"Ion\"}},{\"pattern\":{\"subject\":\"?y\",\"relation\":\"PARENT_OF\",\"object\":\"?z\"},\"match\":{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Maria\"},\"proof\":{\"goal\":{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Maria\"},\"steps\":[{\"fact\":{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Maria\"},\"justification\":\"direct_match\"}],\"valid\":true}}],\"rule\":\"uncle_aunt_rule\",\"valid\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Vasile\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 1 (-), 5/7 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Is Ion an ancestor of Andrei?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Ion is an ancestor of Andrei through the chain: Ion is parent of Maria (so ancestor), and Maria is parent of Andrei (so Ion is ancestor of Andrei too).",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"method\":\"composition\",\"rule\":\"ancestor_transitive_rule\",\"confidence\":0.9,\"proof\":{\"goal\":{\"subject\":\"Ion\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Andrei\"},\"steps\":[{\"pattern\":{\"subject\":\"?x\",\"relation\":\"ANCESTOR_OF\",\"object\":\"?y\"},\"match\":{\"subject\":\"Ion\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Maria\",\"derivedBy\":\"ancestor_direct_rule\"}},{\"pattern\":{\"subject\":\"?y\",\"relation\":\"PARENT_OF\",\"object\":\"?z\"},\"match\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"proof\":{\"goal\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"steps\":[{\"fact\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"justification\":\"direct_match\"}],\"valid\":true}}],\"rule\":\"ancestor_transitive_rule\",\"valid\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Ion\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 1 (-), 9/13 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Are Andrei and Cosmin cousins?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Andrei and Cosmin are cousins. They share Ion as a common grandparent (Ion->Maria->Andrei and Ion's sibling Vasile->Cosmin implies shared great-grandparent level), and they are not siblings.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_evidence\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"Andrei\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Weak NL Match (DSL UNKNOWN but 1 text indicators for TRUE_CERTAIN)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Why is Ion the grandparent of Andrei?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Proof chain: 1) Ion PARENT_OF Maria (fact), 2) Maria PARENT_OF Andrei (fact), 3) By grandparent_rule: Ion GRANDPARENT_OF Andrei (derived).",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"proven\":true,\"method\":\"composition\",\"proof\":{\"goal\":{\"subject\":\"Ion\",\"relation\":\"GRANDPARENT_OF\",\"object\":\"Andrei\"},\"steps\":[{\"pattern\":{\"subject\":\"?x\",\"relation\":\"PARENT_OF\",\"object\":\"?y\"},\"match\":{\"subject\":\"Ion\",\"relation\":\"PARENT_OF\",\"object\":\"Maria\"}},{\"pattern\":{\"subject\":\"?y\",\"relation\":\"PARENT_OF\",\"object\":\"?z\"},\"match\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"proof\":{\"goal\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"steps\":[{\"fact\":{\"subject\":\"Maria\",\"relation\":\"PARENT_OF\",\"object\":\"Andrei\"},\"justification\":\"direct_match\"}],\"valid\":true}}],\"rule\":\"grandparent_rule\",\"valid\":true},\"confidence\":0.9}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 8/13 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "How many new facts can be derived from forward chaining?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Forward chaining derives multiple new facts: grandparent relations (Ion and Ana are grandparents of Andrei and Elena), uncle relations (Vasile is uncle of Maria), ancestor relations (multiple levels), and cousin relations.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"derived\":[{\"subject\":\"Ion\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Maria\",\"derivedBy\":\"ancestor_direct_rule\"},{\"subject\":\"Maria\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Andrei\",\"derivedBy\":\"ancestor_direct_rule\"},{\"subject\":\"Maria\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Elena\",\"derivedBy\":\"ancestor_direct_rule\"},{\"subject\":\"Ana\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Maria\",\"derivedBy\":\"ancestor_direct_rule\"},{\"subject\":\"Vasile\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Cosmin\",\"derivedBy\":\"ancestor_direct_rule\"},{\"subject\":\"Ion\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Andrei\",\"derivedBy\":\"transitive_closure\"},{\"subject\":\"Ion\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Elena\",\"derivedBy\":\"transitive_closure\"},{\"subject\":\"Ana\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Andrei\",\"derivedBy\":\"transitive_closure\"},{\"subject\":\"Ana\",\"relation\":\"ANCESTOR_OF\",\"object\":\"Elena\",\"derivedBy\":\"transitive_closure\"},{\"subject\":\"Ion\",\"relation\":\"SIBLING_OF\",\"object\":\"Vasile\",\"derivedBy\":\"symmetric_closure\"},{\"subject\":\"Ana\",\"relation\":\"MARRIED_TO\",\"object\":\"Ion\",\"derivedBy\":\"symmetric_closure\"}],\"count\":11,\"originalCount\":163}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 2/21 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Is Andrei a sibling of Elena?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Andrei and Elena are siblings - they share the same parent Maria.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_evidence\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"Andrei\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Weak NL Match (DSL UNKNOWN but 1 text indicators for TRUE_CERTAIN)",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_08_default_reasoning",
      "name": "Default Reasoning with Exceptions (Non-Monotonic Logic)",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Can Tweety fly?",
          "expectedTruth": "TRUE_DEFAULT",
          "expectedNatural": "Yes, Tweety can fly. Tweety is a bird, and birds can fly by default. Tweety is not one of the exceptions (penguin, ostrich, kiwi, emu).",
          "actualResponse": "{\"truth\":\"TRUE_DEFAULT\",\"method\":\"default\",\"confidence\":0.8,\"assumptions\":[\"Tweety is a typical bird\"],\"proof\":{\"goal\":{\"subject\":\"Tweety\",\"relation\":\"CAN\",\"object\":\"fly\"},\"steps\":[{\"rule\":\"birds_fly\",\"justification\":\"default_rule\"},{\"assumption\":\"Tweety IS_A bird\",\"justification\":\"type_check\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Tweety\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_DEFAULT, got TRUE_DEFAULT (method: default)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Can Opus fly?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Opus cannot fly. Although Opus is a bird (penguins are birds), penguins are an exception to the flying rule.",
          "actualResponse": "{\"truth\":\"FALSE\",\"method\":\"default\",\"reason\":\"exception_applies\",\"exception\":\"penguin\",\"proof\":{\"goal\":{\"subject\":\"Opus\",\"relation\":\"CAN\",\"object\":\"fly\"},\"steps\":[{\"exception\":\"penguin\",\"justification\":\"blocked_by_exception\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Opus\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected FALSE, got FALSE (method: default)",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Does Rex give birth to live young?",
          "expectedTruth": "TRUE_DEFAULT",
          "expectedNatural": "Yes, Rex gives birth to live young. Rex is a dog, dogs are mammals, and mammals give birth to live young by default. Dogs are not an exception.",
          "actualResponse": "{\"truth\":\"TRUE_DEFAULT\",\"method\":\"default\",\"confidence\":0.8,\"assumptions\":[\"Rex is a typical mammal\"],\"proof\":{\"goal\":{\"subject\":\"Rex\",\"relation\":\"GIVES_BIRTH_TO\",\"object\":\"live_young\"},\"steps\":[{\"rule\":\"mammals_live_birth\",\"justification\":\"default_rule\"},{\"assumption\":\"Rex IS_A mammal\",\"justification\":\"type_check\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Rex\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_DEFAULT, got TRUE_DEFAULT (method: default)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Does Perry give birth to live young?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Perry does not give birth to live young. Perry is a platypus, and platypuses are an exception - they lay eggs despite being mammals.",
          "actualResponse": "{\"truth\":\"FALSE\",\"method\":\"default\",\"reason\":\"exception_applies\",\"exception\":\"platypus\",\"proof\":{\"goal\":{\"subject\":\"Perry\",\"relation\":\"GIVES_BIRTH_TO\",\"object\":\"live_young\"},\"steps\":[{\"exception\":\"platypus\",\"justification\":\"blocked_by_exception\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Perry\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 1 (-), 4/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Does Nemo live in water?",
          "expectedTruth": "TRUE_DEFAULT",
          "expectedNatural": "Yes, Nemo lives in water. Nemo is a goldfish, goldfish are fish, and fish live in water by default.",
          "actualResponse": "{\"truth\":\"TRUE_DEFAULT\",\"method\":\"default\",\"confidence\":0.8,\"assumptions\":[\"Nemo is a typical fish\"],\"proof\":{\"goal\":{\"subject\":\"Nemo\",\"relation\":\"LIVES_IN\",\"object\":\"water\"},\"steps\":[{\"rule\":\"fish_aquatic\",\"justification\":\"default_rule\"},{\"assumption\":\"Nemo IS_A fish\",\"justification\":\"type_check\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Nemo\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_DEFAULT, got TRUE_DEFAULT (method: default)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Does Muddy live in water?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, or at least not exclusively. Muddy is a mudskipper, and mudskippers are an exception - they can survive on land.",
          "actualResponse": "{\"truth\":\"FALSE\",\"method\":\"default\",\"reason\":\"exception_applies\",\"exception\":\"mudskipper\",\"proof\":{\"goal\":{\"subject\":\"Muddy\",\"relation\":\"LIVES_IN\",\"object\":\"water\"},\"steps\":[{\"exception\":\"mudskipper\",\"justification\":\"blocked_by_exception\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Muddy\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected FALSE, got FALSE (method: default)",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Is Scales cold-blooded?",
          "expectedTruth": "TRUE_DEFAULT",
          "expectedNatural": "Yes, Scales is cold-blooded. Scales is a lizard, lizards are reptiles, and all reptiles are cold-blooded with no exceptions.",
          "actualResponse": "{\"truth\":\"TRUE_DEFAULT\",\"method\":\"default\",\"confidence\":0.8,\"assumptions\":[\"Scales is a typical reptile\"],\"proof\":{\"goal\":{\"subject\":\"Scales\",\"relation\":\"IS\",\"object\":\"cold_blooded\"},\"steps\":[{\"rule\":\"reptiles_cold\",\"justification\":\"default_rule\"},{\"assumption\":\"Scales IS_A reptile\",\"justification\":\"type_check\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Scales\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_DEFAULT, got TRUE_DEFAULT (method: default)",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Can Speedy fly?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Speedy cannot fly. Speedy is an ostrich, and ostriches are an exception to the birds-can-fly rule.",
          "actualResponse": "{\"truth\":\"FALSE\",\"method\":\"default\",\"reason\":\"exception_applies\",\"exception\":\"ostrich\",\"proof\":{\"goal\":{\"subject\":\"Speedy\",\"relation\":\"CAN\",\"object\":\"fly\"},\"steps\":[{\"exception\":\"ostrich\",\"justification\":\"blocked_by_exception\"}],\"defeasible\":true},\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Speedy\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 2 (-), 3/8 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_09_contradictions",
      "name": "Contradiction Detection & Constraint Validation",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Would it be a contradiction to say John was born in New York?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, this would be a contradiction. John is already recorded as born in Chicago, and BORN_IN is a functional relation - a person can only be born in one place.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"wouldContradict\":true,\"reason\":\"FUNCTIONAL_VIOLATION\",\"contradictions\":[{\"type\":\"FUNCTIONAL_VIOLATION\",\"severity\":\"ERROR\",\"subject\":\"John\",\"relation\":\"BORN_IN\",\"values\":[\"Chicago\",\"New_York\"],\"facts\":[{\"subject\":\"John\",\"relation\":\"BORN_IN\",\"object\":\"Chicago\"},{\"subject\":\"John\",\"relation\":\"BORN_IN\",\"object\":\"New_York\"}],\"explanation\":\"John has multiple BORN_IN values: Chicago, New_York (BORN_IN should be single-valued)\",\"resolution\":[\"Retract \\\"John BORN_IN Chicago\\\"\",\"Retract \\\"John BORN_IN New_York\\\"\"]}],\"proposedFact\":{\"subject\":\"John\",\"relation\":\"BORN_IN\",\"object\":\"New_York\"}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 5/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Would it be a contradiction to say John has a second biological mother named Susan?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, this would be a contradiction. John already has Mary as biological mother, and BIOLOGICAL_MOTHER is functional.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"wouldContradict\":true,\"reason\":\"FUNCTIONAL_VIOLATION\",\"contradictions\":[{\"type\":\"FUNCTIONAL_VIOLATION\",\"severity\":\"ERROR\",\"subject\":\"John\",\"relation\":\"BIOLOGICAL_MOTHER\",\"values\":[\"Mary\",\"Susan\"],\"facts\":[{\"subject\":\"John\",\"relation\":\"BIOLOGICAL_MOTHER\",\"object\":\"Mary\"},{\"subject\":\"John\",\"relation\":\"BIOLOGICAL_MOTHER\",\"object\":\"Susan\"}],\"explanation\":\"John has multiple BIOLOGICAL_MOTHER values: Mary, Susan (BIOLOGICAL_MOTHER should be single-valued)\",\"resolution\":[\"Retract \\\"John BIOLOGICAL_MOTHER Mary\\\"\",\"Retract \\\"John BIOLOGICAL_MOTHER Susan\\\"\"]}],\"proposedFact\":{\"subject\":\"John\",\"relation\":\"BIOLOGICAL_MOTHER\",\"object\":\"Susan\"}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 3/7 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Would it be a contradiction to say John is dead?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, this would be a contradiction. John is currently recorded as alive, and alive and dead are disjoint - a person cannot be both.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"wouldContradict\":true,\"reason\":\"DISJOINT_VIOLATION\",\"contradictions\":[{\"type\":\"DISJOINT_VIOLATION\",\"severity\":\"ERROR\",\"entity\":\"John\",\"types\":[\"alive\",\"dead\"],\"facts\":[{\"subject\":\"John\",\"relation\":\"IS_A\",\"object\":\"alive\"},{\"subject\":\"John\",\"relation\":\"IS_A\",\"object\":\"dead\"},{\"subject\":\"alive\",\"relation\":\"DISJOINT_WITH\",\"object\":\"dead\"}],\"explanation\":\"John cannot be both alive and dead (they are disjoint)\",\"resolution\":[\"Retract a fact making John a alive\",\"Retract a fact making John a dead\"]},{\"type\":\"DISJOINT_VIOLATION\",\"severity\":\"ERROR\",\"entity\":\"John\",\"types\":[\"alive\",\"dead\"],\"facts\":[{\"subject\":\"John\",\"relation\":\"IS_A\",\"object\":\"alive\"},{\"subject\":\"John\",\"relation\":\"IS_A\",\"object\":\"dead\"},{\"subject\":\"alive\",\"relation\":\"DISJOINT_WITH\",\"object\":\"dead\"}],\"explanation\":\"John cannot be both alive and dead (they are disjoint)\",\"resolution\":[\"Retract a fact making John a alive\",\"Retract a fact making John a dead\"]}],\"proposedFact\":{\"subject\":\"John\",\"relation\":\"IS_A\",\"object\":\"dead\"}}",
          "passed": true,
          "matchReason": "NL Match: 3 indicators (+), 3 (-), 4/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Is the current theory consistent?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, the current theory is consistent. No functional constraints are violated, no disjoint types are assigned to the same entity, and cardinality constraints are satisfied.",
          "actualResponse": "{\"consistent\":true,\"issues\":[],\"scope\":\"all\",\"factCount\":170}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/13 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Can John have a fourth doctor assigned?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, if John already has 3 doctors, adding a fourth would violate the cardinality constraint (max 3 doctors per patient).",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"wouldContradict\":true,\"reason\":\"CARDINALITY_MAX_VIOLATION\",\"contradictions\":[{\"type\":\"CARDINALITY_MAX_VIOLATION\",\"severity\":\"ERROR\",\"subject\":\"John\",\"relation\":\"HAS_DOCTOR\",\"count\":4,\"maximum\":3,\"facts\":[{\"subject\":\"John\",\"relation\":\"HAS_DOCTOR\",\"object\":\"DrSmith\"},{\"subject\":\"John\",\"relation\":\"HAS_DOCTOR\",\"object\":\"DrJones\"},{\"subject\":\"John\",\"relation\":\"HAS_DOCTOR\",\"object\":\"DrWilson\"},{\"subject\":\"John\",\"relation\":\"HAS_DOCTOR\",\"object\":\"DrBrown\"}],\"explanation\":\"John has 4 HAS_DOCTOR relations, but maximum is 3\",\"resolution\":[\"Retract \\\"John HAS_DOCTOR DrSmith\\\"\",\"Retract \\\"John HAS_DOCTOR DrJones\\\"\",\"Retract \\\"John HAS_DOCTOR DrWilson\\\"\",\"Retract \\\"John HAS_DOCTOR DrBrown\\\"\"]}],\"proposedFact\":{\"subject\":\"John\",\"relation\":\"HAS_DOCTOR\",\"object\":\"DrBrown\"}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 2/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Would saying 'cat IS_A mammal AND cat IS_A fish' be a contradiction?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, this would be a contradiction. Mammals and fish are disjoint categories - nothing can be both a mammal and a fish.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"wouldContradict\":true,\"reason\":\"DISJOINT_VIOLATION\",\"contradictions\":[{\"type\":\"DISJOINT_VIOLATION\",\"severity\":\"ERROR\",\"entity\":\"cat\",\"types\":[\"mammal\",\"fish\"],\"facts\":[{\"subject\":\"cat\",\"relation\":\"IS_A\",\"object\":\"mammal\"},{\"subject\":\"cat\",\"relation\":\"IS_A\",\"object\":\"fish\"}],\"explanation\":\"cat cannot be both mammal and fish (they are disjoint)\",\"resolution\":[\"Retract a fact making cat a mammal\",\"Retract a fact making cat a fish\"]},{\"type\":\"DISJOINT_VIOLATION\",\"severity\":\"ERROR\",\"entity\":\"cat\",\"types\":[\"cat\",\"fish\"],\"facts\":[{\"subject\":\"cat\",\"relation\":\"IS_A\",\"object\":\"fish\"}],\"explanation\":\"cat cannot be a fish because cat and fish are disjoint types\",\"resolution\":[\"Retract the fact \\\"cat IS_A fish\\\"\"]}],\"proposedFact\":{\"subject\":\"cat\",\"relation\":\"IS_A\",\"object\":\"fish\"}}",
          "passed": true,
          "matchReason": "NL Match: 3 indicators (+), 3 (-), 3/8 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Can John's diabetes diagnosis be both confirmed and ruled out?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, this would be a contradiction. The diagnosis is already confirmed, and confirmed and ruled_out are disjoint states.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"wouldContradict\":true,\"reason\":\"FUNCTIONAL_VIOLATION\",\"contradictions\":[{\"type\":\"FUNCTIONAL_VIOLATION\",\"severity\":\"ERROR\",\"subject\":\"diabetes_diagnosis\",\"relation\":\"STATUS\",\"values\":[\"confirmed\",\"ruled_out\"],\"facts\":[{\"subject\":\"diabetes_diagnosis\",\"relation\":\"STATUS\",\"object\":\"confirmed\"},{\"subject\":\"diabetes_diagnosis\",\"relation\":\"STATUS\",\"object\":\"ruled_out\"}],\"explanation\":\"diabetes_diagnosis has multiple STATUS values: confirmed, ruled_out (STATUS should be single-valued)\",\"resolution\":[\"Retract \\\"diabetes_diagnosis STATUS confirmed\\\"\",\"Retract \\\"diabetes_diagnosis STATUS ruled_out\\\"\"]}],\"proposedFact\":{\"subject\":\"diabetes_diagnosis\",\"relation\":\"STATUS\",\"object\":\"ruled_out\"}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 1 (-), 5/9 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Why would having two birth places be a contradiction?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "BORN_IN is registered as a functional relation, meaning each subject can have at most one value for this relation. A person can only be born in one place - this is a real-world constraint that the system enforces.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"wouldContradict\":true,\"reason\":\"FUNCTIONAL_VIOLATION\",\"contradictions\":[{\"type\":\"FUNCTIONAL_VIOLATION\",\"severity\":\"ERROR\",\"subject\":\"TestPerson\",\"relation\":\"BORN_IN\",\"values\":[\"PlaceA\",\"PlaceB\"],\"facts\":[{\"subject\":\"TestPerson\",\"relation\":\"BORN_IN\",\"object\":\"PlaceA\"},{\"subject\":\"TestPerson\",\"relation\":\"BORN_IN\",\"object\":\"PlaceB\"}],\"explanation\":\"TestPerson has multiple BORN_IN values: PlaceA, PlaceB (BORN_IN should be single-valued)\",\"resolution\":[\"Retract \\\"TestPerson BORN_IN PlaceA\\\"\",\"Retract \\\"TestPerson BORN_IN PlaceB\\\"\"]}],\"proposedFact\":{\"subject\":\"TestPerson\",\"relation\":\"BORN_IN\",\"object\":\"PlaceB\"}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 6/14 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_10_theory_layers",
      "name": "Theory Layering & Advanced Counterfactual Reasoning",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "In a world where water boiled at 50 degrees, would it be liquid at room temperature?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, water would still be liquid at room temperature (20 degrees) if it boiled at 50 degrees, since 20 < 50.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"water\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "What if there was no oxygen - could fire exist?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No, fire could not exist without oxygen. Fire requires oxygen to burn, so in a world without oxygen, fire is impossible.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"fire\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "What if Napoleon had won Waterloo - would the French Empire have continued?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Plausibly yes. If Napoleon had won Waterloo instead of losing, the French Empire might have continued since its end was a consequence of the defeat.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Napoleon\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Create a 'no_gravity' layer and check if objects would fall",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "In the no-gravity layer, objects would not fall because we retracted the causal relationship. After THEORY_POP, normal gravity rules apply again.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"gravity\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "After popping the no-gravity layer, do objects fall again?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, after popping the counterfactual layer, we return to base reality where gravity causes objects to fall.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"gravity\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/11 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Can we have nested counterfactual layers?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, nested layers work. In inner layer, both changes are visible. After popping inner, water boils at 100 again but Napoleon still won. After popping outer, both revert to base reality.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"water\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "What if smartphones didn't require electricity?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "If smartphones didn't require electricity, they could theoretically function without power. However, this counterfactual removes a fundamental dependency.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"smartphone\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Verify layer isolation - changes in a layer should not affect base theory",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "In the test layer, Roman Empire fell in 500 AD (as asserted). After popping, the base theory shows 476 AD unchanged. Layers are properly isolated.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"Roman_Empire\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 2/12 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_11_recursive_small",
      "name": "Recursive Planning Problems - Small Scale",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "In the wolf-goat-cabbage problem, what happens if the farmer leaves the wolf and goat alone?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "If the farmer leaves the wolf and goat alone on either bank, the wolf will eat the goat. This is a constraint that must be avoided.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"wolf_and_goat_alone\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/8 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "What should be the first move in the wolf-goat-cabbage problem?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "The farmer must take the goat first. This is the only safe first move because: leaving wolf+goat alone = goat eaten, leaving goat+cabbage alone = cabbage eaten, but wolf+cabbage alone is safe.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"first_move\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/16 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Can the wolf-goat-cabbage problem be solved in fewer than 7 crossings?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, 7 crossings is the minimum. The constraints force specific moves, and there is no shorter valid sequence.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"wolf_goat_cabbage\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "How many moves are needed for Towers of Hanoi with 3 disks?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "7 moves are needed. The formula is 2^n - 1, so for n=3: 2^3 - 1 = 8 - 1 = 7.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"hanoi_3_disks\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "In Hanoi, can a large disk be placed on a small disk?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, a larger disk cannot be placed on a smaller disk. This is a fundamental rule of the Towers of Hanoi.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"large_on_small\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/8 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "What is the recursive pattern for solving Towers of Hanoi?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "The recursive pattern is: 1) Move n-1 disks from source to auxiliary peg, 2) Move the largest disk from source to target peg, 3) Move n-1 disks from auxiliary to target peg. Base case: 1 disk = 1 move.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"hanoi\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "How many moves are needed for Towers of Hanoi with 4 disks?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "15 moves are needed. Using the formula 2^n - 1: 2^4 - 1 = 16 - 1 = 15.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"hanoi_4_disks\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "In wolf-goat-cabbage, why must the farmer bring the goat back on the 4th crossing?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "After taking the wolf to the right bank (crossing 3), the farmer must bring the goat back because leaving wolf+goat together on the right bank would result in the wolf eating the goat. The goat acts as a 'shuttle' that must be carefully managed.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"crossing_4\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/18 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_12_recursive_large",
      "name": "Recursive Planning Problems - Large Scale",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "How many moves are needed for Towers of Hanoi with 15 disks?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "32,767 moves are needed. Using 2^15 - 1 = 32768 - 1 = 32767.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"hanoi_15_disks\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "How many moves are needed for Towers of Hanoi with 20 disks?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "1,048,575 moves are needed. Using 2^20 - 1 = 1048576 - 1 = 1048575. This is over 1 million moves!",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"hanoi_20_disks\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "If each Hanoi move takes 1 second, how long would 20 disks take?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "1,048,575 seconds, which is approximately 12.1 days (1048575 / 86400 ≈ 12.14 days).",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"hanoi_20_time_seconds\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "What is the 15th Fibonacci number?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "The 15th Fibonacci number (F(15)) is 610. Sequence: ...233, 377, 610...",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"fibonacci_15\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "What is the 20th Fibonacci number?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "The 20th Fibonacci number (F(20)) is 6765.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"fibonacci_20\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "What is 15 factorial?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "15! = 1,307,674,368,000 (over 1.3 trillion).",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"factorial_15\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "How does the number of Hanoi moves grow when we add one more disk?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "The number of moves approximately doubles with each additional disk. Specifically, moves(n+1) = 2 × moves(n) + 1. This is exponential growth O(2^n).",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"exponential_growth\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 2/12 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Is it feasible to physically solve Towers of Hanoi with 20 disks?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "No, it is not practically feasible. At 1 move per second, it would take over 12 days of continuous operation. At a more realistic 1 move per 5 seconds, it would take about 60 days. The exponential nature makes large instances impractical.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"hanoi_20\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: direct)",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_13_variable_composition",
      "name": "Variable Chaining & Multi-Step Composition",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Find all courses Alice is enrolled in and check if the list is non-empty.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Alice has enrollments. She is enrolled in Math101 and Physics201.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Does Bob have at least one enrollment?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Bob has enrollments - he is enrolled in Math101 and Chemistry101.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Is Alice enrolled in both Math101 and Physics201?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Alice is enrolled in both Math101 and Physics201.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Can Alice take Physics301 (which requires Physics201)?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Alice can take Physics301 because she has completed Physics201 which is its prerequisite.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/7 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Can Bob take Math201 (which requires Math101)?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Bob can take Math201 because he has completed Math101.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Does Employee_1 (a Manager) have access to both finance_data and hr_data?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Employee_1 is a Manager and Managers can access both finance_data and hr_data.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Can Employee_2 (an Engineer) access finance_data?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No, Employee_2 is an Engineer and Engineers cannot access finance_data (only tech_data and code_repo).",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_evidence\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"Engineer\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Find all symptoms Patient_A has and check if flu could explain them all.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Flu causes all three symptoms that Patient_A has (fever, cough, fatigue), making it a likely diagnosis.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/10 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q9",
          "question": "Does Cold explain all of Patient_A's symptoms?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Cold only causes cough and mild_fever, not fever and fatigue. Cold doesn't explain all symptoms.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/8 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q10",
          "question": "Combine Alice's and Bob's course lists - do they have any combined enrollments?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, combined Alice and Bob have enrollments (Alice: Math101, Physics201; Bob: Math101, Chemistry101).",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/8 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q11",
          "question": "In a hypothetical scenario where Alice drops Math101, does she still have any enrollments?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, even if Alice drops Math101, she still has Physics201.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q12",
          "question": "After hypothetically removing Math101, does Alice still have enrollments?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Alice still has Physics201 enrollment even after removing Math101.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/7 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 12,
      "failed": 0
    },
    {
      "id": "suite_14_error_edge_cases",
      "name": "Error Handling & Edge Cases",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Is XyzzyUnknown123 an animal?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "Unknown - XyzzyUnknown123 is not a known concept in the knowledge base.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"animal\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "What facts do we have about NonExistent?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No facts exist about NonExistent. The result list is empty.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "How many facts do we have about UnknownThing?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Zero facts - UnknownThing is not in the knowledge base.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "What is the first fact about Dog?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "The fact 'Dog IS_A animal' - Dog has at least one known fact.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "What is the first fact about NonExistent?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No facts exist about NonExistent.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Is Cat both an animal AND a plant?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No - Cat is an animal but there is no fact saying Cat is a plant.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Is Cat an animal OR a plant?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes - Cat is an animal (even though we don't know about plant).",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Does merging two empty lists give an empty list?",
          "expectedTruth": "FALSE",
          "expectedNatural": "The merged list is empty since both source lists are empty.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q9",
          "question": "What happens when we negate an UNKNOWN result?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "Negating UNKNOWN should still be UNKNOWN - we can't determine the negation of something we don't know.",
          "actualResponse": "{\"truth\":\"UNKNOWN\"}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q10",
          "question": "Can we filter an empty list without errors (returns empty)?",
          "expectedTruth": "FALSE",
          "expectedNatural": "Filtering an empty list returns an empty list, so NONEMPTY is FALSE.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/7 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q11",
          "question": "Does THEORY_PUSH and THEORY_POP with no changes work correctly?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, pushing and popping an empty theory layer should not affect existing facts. Cat is still an animal.",
          "actualResponse": "{\"ok\":true}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/11 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q12",
          "question": "Can we check for contradictions in a consistent knowledge base?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No contradictions found - the knowledge base is consistent.",
          "actualResponse": "{\"consistent\":true,\"contradictions\":[],\"summary\":\"No contradictions found.\",\"factCount\":160}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong FALSE indicators",
          "hasDslExplanation": false
        }
      ],
      "passed": 12,
      "failed": 0
    },
    {
      "id": "suite_15_masks",
      "name": "Masking & Dimension Registry",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Is Tiger a mammal when asking with an ontology partition mask?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Even with an ontology mask applied, Tiger is known to be a mammal.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"mammal\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0},\"maskSpec\":\"ontology\"}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Is Bonsai a plant when masking a specific dimension by name?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Bonsai is a plant; masking the Physicality axis should not remove the known fact.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"plant\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0},\"maskSpec\":\"Physicality\"}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Is Eagle a bird when asking with an axiology partition mask?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Axiology mask affects moral dimensions, not taxonomic facts about animals.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"bird\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0},\"maskSpec\":\"axiology\"}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Is Theft prohibited when asking with ontology mask (not axiology)?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Ontology mask doesn't hide axiology facts like prohibitions.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"Theft\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0},\"maskSpec\":\"ontology\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Is Honesty a virtue when asking with combined mask?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Even with combined masks, the basic IS_A fact is preserved.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"virtue\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0},\"maskSpec\":\"ontology,axiology\"}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Is Rock a mineral without any mask (baseline)?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Rock is known to be a mineral.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"mineral\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/2 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Is Helping a good action when masking axiology partitions?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. The IS_A relation is preserved even when axiology partitions are masked.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"good_action\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0},\"maskSpec\":\"axiology\"}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Does Water have property liquid without any mask (baseline)?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Baseline ASK returns the asserted fact.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"Water\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_16_memory_forgetting",
      "name": "Memory Management: Protect & Forget",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "After protecting KeepMe and forgetting low-usage concepts, is KeepMe still known as important?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. KeepMe was protected before forgetting, so it should still be known as important.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"important_thing\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "After the same forgetting pass, is DropMe still known as temporary?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No information remains about DropMe; it was removed by forgetting.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"temporary_thing\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Can we protect CoreConcept and verify it survives a higher threshold forget?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Protected concepts survive any threshold-based forgetting.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"essential_thing\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "After unprotecting and forgetting, is a previously protected concept gone?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. After unprotecting, TempData is subject to forgetting.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"throwaway_thing\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Does Archive survive when we forget with a very high threshold?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Protected Archive survives even aggressive forgetting thresholds.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"old_data\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Is Critical still known after protecting it multiple times?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Multiple PROTECT calls are idempotent; concept remains protected.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"vital_thing\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "After forgetting, is Scratch1 still known?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. Unprotected low-usage concepts are forgotten.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"scratch_item\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Verify Scratch2 is also gone after the same forget pass.",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. Scratch2 was also unprotected and had low usage.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"scratch_item\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_17_theory_storage",
      "name": "Theory Storage: Save, Retract, Reload",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "After saving and retracting, is DiskA still recorded as part of the Computer?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. After retracting, the fact should no longer be present until reloaded.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"DiskA\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "After reloading the saved theory, is DiskA part of the Computer again?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Loading the saved theory restores the fact that DiskA is part of the Computer.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"DiskA\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/6 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Can we save a theory under a different name and reload it?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. The theory was saved as memory_backup and successfully reloaded.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"MemoryB\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "After saving, retracting, but NOT reloading, is CPUC part of Workstation?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. Without reloading, the retracted fact stays gone.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"CPUC\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Is NetworkD connected to Router after save and reload cycle?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. The full save-retract-reload cycle restores the fact.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"NetworkD\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Does PowerSupply still power Computer without any retraction?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Facts not retracted remain in the store.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"PowerSupply\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "After multiple retracts without save, can we still query CoolingFan?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. The fact was retracted and there's no saved backup to restore from.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"CoolingFan\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Can we overwrite a saved theory and reload the new version?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. The second save overwrites the first; reload gets the newer version.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"test_item\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/6 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_18_polarity_decision",
      "name": "Polarity Decision with FACTS_MATCHING",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "For EU, does negative evidence dominate for drugA?",
          "expectedTruth": "FALSE",
          "expectedNatural": "EU has a prohibition, so the decision should be negative.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "For US, does positive evidence win for drugA?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "US has a permission fact and no prohibition, so the decision should be true.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Is drugB permitted in EU (only positive evidence)?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. DrugB has only permission facts, no prohibitions.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Is drugB permitted in US as well?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. DrugB is permitted in US too.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/2 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Is drugC prohibited in EU (only negative evidence)?",
          "expectedTruth": "FALSE",
          "expectedNatural": "Yes, decision is FALSE because drugC is prohibited in EU.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Is drugC also prohibited in US?",
          "expectedTruth": "FALSE",
          "expectedNatural": "Yes, decision is FALSE because drugC is prohibited in US too.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "What about productX in US (positive only)?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "ProductX is permitted in US.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/2 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "What about productX in Asia (negative only)?",
          "expectedTruth": "FALSE",
          "expectedNatural": "ProductX is prohibited in Asia.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_19_output_format",
      "name": "Output Formatting: TO_NATURAL and TO_JSON",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Convert a true ASK result to natural language.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, this is definitely true.",
          "actualResponse": "{\"text\":\"Yes, this is definitely true.\"}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong TRUE_CERTAIN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Convert an unknown ASK result to natural language.",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "Unknown - the concept is not in the KB.",
          "actualResponse": "{\"text\":\"Truth value: UNKNOWN\"}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Convert a TRUE_CERTAIN result about Grass to natural language.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Grass is a green thing.",
          "actualResponse": "{\"text\":\"Yes, this is definitely true.\"}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong TRUE_CERTAIN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Convert Sun's hot property query to JSON format.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "JSON output with truth: TRUE_CERTAIN",
          "actualResponse": "{\"json\":\"{\\\"truth\\\":\\\"TRUE_CERTAIN\\\",\\\"confidence\\\":1,\\\"method\\\":\\\"direct\\\",\\\"band\\\":\\\"TRUE_CERTAIN\\\",\\\"provenance\\\":{\\\"conceptId\\\":\\\"Sun\\\",\\\"distance\\\":0,\\\"scepticRadius\\\":0,\\\"optimistRadius\\\":0}}\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Format Ice's cold property result.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Ice has property cold.",
          "actualResponse": "{\"text\":\"Yes, this is definitely true.\"}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong TRUE_CERTAIN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Convert Fire's ability to burn to natural language.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Fire can burn.",
          "actualResponse": "{\"text\":\"Yes, this is definitely true.\"}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong TRUE_CERTAIN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Convert a FALSE result (wrong fact) to natural language.",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No, Bird cannot swim (not in KB).",
          "actualResponse": "{\"text\":\"Truth value: UNKNOWN\"}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Convert Fish swimming ability to JSON.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "JSON output confirming Fish can swim.",
          "actualResponse": "{\"json\":\"{\\\"truth\\\":\\\"TRUE_CERTAIN\\\",\\\"confidence\\\":1,\\\"method\\\":\\\"direct\\\",\\\"band\\\":\\\"TRUE_CERTAIN\\\",\\\"provenance\\\":{\\\"conceptId\\\":\\\"Fish\\\",\\\"distance\\\":0,\\\"scepticRadius\\\":0,\\\"optimistRadius\\\":0}}\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_20_format_summarize",
      "name": "FORMAT & SUMMARIZE with FACTS_MATCHING",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Summarize known fruits and format the result status.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "There are fruits; the list is non-empty.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Check a missing category and format the negative outcome.",
          "expectedTruth": "FALSE",
          "expectedNatural": "There are no spaceships in the facts.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Count all vehicles and check if non-empty.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "There are vehicles in the KB.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/2 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Summarize vehicles with maxItems=1.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Vehicle summary created; list is non-empty.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Match all animals and verify non-empty.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Animals exist in the KB.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/2 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Check for non-existent robots category.",
          "expectedTruth": "FALSE",
          "expectedNatural": "No robots in the KB.",
          "actualResponse": "{\"truth\":\"FALSE\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/1 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Summarize fruits with maxItems=4 (all of them).",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "All 4 fruits summarized; list is non-empty.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Match specific subject Apple and check.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Apple has at least one IS_A fact.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_21_boost_forget",
      "name": "Boost Before Forgetting",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Create and boost ImportantNote, then forget - should survive.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. ImportantNote was asserted, boosted to 5, above threshold 3.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"note\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Create ScratchNote without boosting, then forget - should be removed.",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. ScratchNote had low usage (1) and was forgotten.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"scratch\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Create and boost CriticalItem with high value, use high threshold.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. CriticalItem was boosted to 20, above threshold 10.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"item\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "Create TempItem, don't boost, use high threshold - should be removed.",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. TempItem was not boosted and is below threshold.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"temp\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Create HighPriority, boost to exactly threshold, should survive.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Boost of 4 + initial usage of 1 = 5, equals threshold 5.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"task\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Create LowPriority, boost above threshold, should survive.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. LowPriority boosted to 8+1=9, above threshold 5.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"work\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Create Essential, boost to 5, threshold 5, should survive.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Essential boosted to 5+1=6, above threshold 5.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"concept\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "Create Disposable, no boost, threshold 5, should be removed.",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. Disposable had no boost (usage=1), below threshold 5.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"junk\",\"reason\":\"NO_CONCEPT\"}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_22_merge_theory_patch",
      "name": "Merge a Saved Patch into the Base Theory",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "After saving and retracting, is RobotA still using BatteryX?",
          "expectedTruth": "UNKNOWN",
          "expectedNatural": "No. After retraction, the fact should not be present.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"unknown_subject\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"RobotA\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "Pattern Match: Found strong UNKNOWN indicators",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "After merging the saved patch, does RobotA use BatteryX again?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. MERGE_THEORY should bring back the saved fact.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"RobotA\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Save, retract, merge cycle for RobotB and BatteryY.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Complete cycle restores the fact.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"RobotB\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "After saving, can we verify the theory was saved successfully?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. The fact is still present after saving (save doesn't remove).",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"DroneC\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/7 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Now merge the drone patch and verify restoration.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Merging restores the DroneC fact.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"DroneC\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 1/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "Verify VehicleD uses FuelCell without any retraction (baseline).",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Untouched facts remain accessible.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"VehicleD\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/4 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Save multiple patches and merge them in sequence.",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. First patch merged restores ServerE.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"ServerE\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "After merging laptop_patch, is LaptopF restored?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes. Second patch merged restores LaptopF.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"LaptopF\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/5 concepts",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_23_summarize_perspective",
      "name": "Structured Report Generation - Climate Policy Analysis",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Section 1: What is the causal chain from greenhouse gases to coastal threats?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q2",
          "question": "Section 1: What releases greenhouse gases?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q3",
          "question": "Section 2: What are the costs vs benefits of climate action?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q4",
          "question": "Section 3: Compile emission reduction methods",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q5",
          "question": "Section 3: What generates clean electricity?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q6",
          "question": "Section 4: What are human health impacts?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q7",
          "question": "Section 5: What policy mechanisms exist?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q8",
          "question": "Section 6 - Synthesis: Prove the full climate argument",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_24_expand_essay",
      "name": "Essay Composition - The Evolution and Impact of Artificial Intelligence",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "INTRODUCTION - Historical Timeline: What are the key milestones in AI history?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q2",
          "question": "INTRODUCTION - Origins: When and where did AI formally begin?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q3",
          "question": "BODY PARAGRAPH 1 - Technical Foundations: What enables modern AI systems?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q4",
          "question": "BODY PARAGRAPH 1 - Learning Methods: How do AI systems acquire knowledge?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q5",
          "question": "BODY PARAGRAPH 2 - Current Applications: What practical tasks does AI perform?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q6",
          "question": "BODY PARAGRAPH 3 - Societal Concerns: What risks and ethical issues does AI raise?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q7",
          "question": "BODY PARAGRAPH 3 - Negative Impacts: What harms might AI cause?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q8",
          "question": "CONCLUSION - Future Outlook: What developments might transform AI?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_25_theory_management",
      "name": "Theory Stack Management and Hypothetical Reasoning",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "Is Alice an employee?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, Alice is an employee.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":0.95,\"method\":\"transitive\",\"depth\":1,\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"employee\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "DSL Match: Expected TRUE_CERTAIN, got TRUE_CERTAIN (method: transitive)",
          "hasDslExplanation": false
        },
        {
          "id": "q2",
          "question": "Where does Bob work?",
          "expectedTruth": "LIST",
          "expectedNatural": "Bob works in Sales.",
          "actualResponse": "[{\"subject\":\"Bob\",\"relation\":\"WORKS_IN\",\"object\":\"Sales\"}]",
          "passed": true,
          "matchReason": "LIST Match: Found 1 result(s)",
          "hasDslExplanation": false
        },
        {
          "id": "q3",
          "question": "Is Alice a manager?",
          "expectedTruth": "FALSE",
          "expectedNatural": "No, Alice is not a manager.",
          "actualResponse": "{\"truth\":\"UNKNOWN\",\"confidence\":0,\"method\":\"no_path\",\"band\":\"FALSE\",\"provenance\":{\"conceptId\":\"manager\",\"distance\":null,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 2 indicators (+), 0 (-), 0/2 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q4",
          "question": "What is the Engineering budget?",
          "expectedTruth": "LIST",
          "expectedNatural": "Engineering has a budget of 100000.",
          "actualResponse": "[{\"subject\":\"Engineering\",\"relation\":\"HAS_BUDGET\",\"object\":\"100000\"}]",
          "passed": true,
          "matchReason": "LIST Match: Found 1 result(s)",
          "hasDslExplanation": false
        },
        {
          "id": "q5",
          "question": "Who are the employees?",
          "expectedTruth": "LIST",
          "expectedNatural": "Alice and Bob are employees.",
          "actualResponse": "[{\"subject\":\"Alice\",\"relation\":\"IS_A\",\"object\":\"employee\"},{\"subject\":\"Bob\",\"relation\":\"IS_A\",\"object\":\"employee\"}]",
          "passed": true,
          "matchReason": "LIST Match: Found 2 result(s)",
          "hasDslExplanation": false
        },
        {
          "id": "q6",
          "question": "What is Alice's salary?",
          "expectedTruth": "LIST",
          "expectedNatural": "Alice earns 70000.",
          "actualResponse": "[{\"subject\":\"Alice\",\"relation\":\"HAS_SALARY\",\"object\":\"70000\"}]",
          "passed": true,
          "matchReason": "LIST Match: Found 1 result(s)",
          "hasDslExplanation": false
        },
        {
          "id": "q7",
          "question": "Can managers access all files?",
          "expectedTruth": "TRUE_CERTAIN",
          "expectedNatural": "Yes, managers can access all files.",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"manager\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/3 concepts",
          "hasDslExplanation": false
        },
        {
          "id": "q8",
          "question": "What can employees access?",
          "expectedTruth": "LIST",
          "expectedNatural": "Employees can access department files.",
          "actualResponse": "[{\"subject\":\"employee\",\"relation\":\"CAN_ACCESS\",\"object\":\"department_files\"}]",
          "passed": true,
          "matchReason": "LIST Match: Found 1 result(s)",
          "hasDslExplanation": false
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_26_tool_planning",
      "name": "Tool Chain Planning - Web Scraping Pipeline",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "PLAN STEP 1: What is the first tool and what parameter does it need?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q2",
          "question": "DEPENDENCY CHECK: What does parse_html need before it can run?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q3",
          "question": "PLAN STEP 3: What inputs does extract_data need?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q4",
          "question": "DATA FLOW: Verify the chain from step_2 to step_4",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q5",
          "question": "GOAL DECOMPOSITION: What sequence produces json_data?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q6",
          "question": "PARAMETER MAPPING: Which steps need configuration parameters?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q7",
          "question": "FINAL STEP: What produces the archive output?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q8",
          "question": "FULL CHAIN: Verify the complete 6-step pipeline",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_27_workflow_planning",
      "name": "Workflow Execution - CI/CD Deployment Pipeline",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "INIT: What parameters does checkout need?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q2",
          "question": "PARALLEL: Which stages take source_code?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q3",
          "question": "DEPENDENCY: What does deploy_prod require?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q4",
          "question": "STEP 5: What does deploy_staging produce?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q5",
          "question": "SECURITY: What does security scan produce?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q6",
          "question": "FAILURE: What triggers pipeline abort?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q7",
          "question": "ROLLBACK: What triggers rollback?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"integration_test_failure\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q8",
          "question": "FINAL: What is produced at pipeline end?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_28_api_orchestration",
      "name": "API Call Sequencing - E-Commerce Order Flow",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "INIT: What is the first API call?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q2",
          "question": "TOKEN: Which calls use the token?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q3",
          "question": "PAYMENT: What does call_5 need?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q4",
          "question": "ORDER: What variables does call_6 use?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q5",
          "question": "OUTPUTS: What does call_4 output?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q6",
          "question": "ERROR: What happens on declined payment?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\",\"confidence\":1,\"method\":\"direct\",\"band\":\"TRUE_CERTAIN\",\"provenance\":{\"conceptId\":\"declined\",\"distance\":0,\"scepticRadius\":0,\"optimistRadius\":0}}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q7",
          "question": "SHIPPING: What does call_7 output?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q8",
          "question": "NOTIFICATION: What does call_8 use?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_29_task_decomposition",
      "name": "Task Decomposition - Recipe Execution Plan",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "PREP: What does prep_2 need and produce?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q2",
          "question": "PARALLEL: What tasks can run parallel during prep?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q3",
          "question": "DEPENDENCY: What must complete before cook_2?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q4",
          "question": "RESOURCE: What tasks use burner_1?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q5",
          "question": "TIMING: How long does cook_1 take?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q6",
          "question": "ASSEMBLY: What does assemble_1 need from cooking?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q7",
          "question": "FINAL: What produces final_dish?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q8",
          "question": "PAN: Which tasks use pan_1?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        }
      ],
      "passed": 8,
      "failed": 0
    },
    {
      "id": "suite_30_resource_planning",
      "name": "Resource Planning - Cloud Infrastructure Provisioning",
      "theoryLoaded": true,
      "queries": [
        {
          "id": "q1",
          "question": "FOUNDATION: What does vpc_main produce?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q2",
          "question": "DEPENDENCY: What depends on vpc_main?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q3",
          "question": "SECURITY: What ports does sg_web allow?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q4",
          "question": "EC2 PREREQS: What must exist before ec2_web_1?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q5",
          "question": "LOAD BALANCER: What does alb_main target?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q6",
          "question": "COST: What is rds_primary monthly cost?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q7",
          "question": "DATABASE DEPS: What does rds_primary need?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        },
        {
          "id": "q8",
          "question": "FINAL: What resource completes infrastructure?",
          "expectedTruth": "TRUE_CERTAIN",
          "actualResponse": "{\"truth\":\"TRUE_CERTAIN\"}",
          "passed": true,
          "matchReason": "NL Match: 1 indicators (+), 0 (-), 0/0 concepts",
          "hasDslExplanation": true
        }
      ],
      "passed": 8,
      "failed": 0
    }
  ]
}