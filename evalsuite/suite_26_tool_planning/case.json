{
  "id": "suite_26_tool_planning",
  "name": "Tool Chain Planning - Web Scraping Pipeline",
  "description": "Tests multi-step tool chain planning with DSL-validatable proof chains. Each query verifies dependencies and data flow using multi-statement DSL that can be executed without LLM.",
  "theory": {
    "natural_language": "TOOL DEFINITIONS: The fetch_url tool takes a url parameter and produces raw_html output. The parse_html tool takes raw_html parameter and produces dom_tree output. The extract_data tool takes dom_tree and selector parameters and produces extracted_list output. The transform_json tool takes extracted_list parameter and produces json_data output. The save_file tool takes json_data and filepath parameters and produces saved_file output. The compress_archive tool takes saved_file parameter and produces archive output. COMPOSITION RULES: To get dom_tree you must first fetch_url then parse_html. To get json_data you must first have extracted_list. EXECUTION SEQUENCE: Step 1 is fetch_url with url parameter. Step 2 is parse_html with raw_html from step 1. Step 3 is extract_data with dom_tree from step 2. Step 4 is transform_json with extracted_list from step 3. Step 5 is save_file with json_data from step 4. Step 6 is compress_archive with saved_file from step 5.",
    "expected_facts": [
      "fetch_url TAKES url",
      "fetch_url PRODUCES raw_html",
      "parse_html TAKES raw_html",
      "parse_html PRODUCES dom_tree",
      "extract_data TAKES dom_tree",
      "extract_data TAKES selector",
      "extract_data PRODUCES extracted_list",
      "transform_json TAKES extracted_list",
      "transform_json PRODUCES json_data",
      "save_file TAKES json_data",
      "save_file TAKES filepath",
      "save_file PRODUCES saved_file",
      "compress_archive TAKES saved_file",
      "compress_archive PRODUCES archive",
      "json_data REQUIRES_FIRST extracted_list",
      "step_1 EXECUTES fetch_url",
      "step_1 WITH_PARAM url",
      "step_2 EXECUTES parse_html",
      "step_2 INPUT_FROM step_1",
      "step_3 EXECUTES extract_data",
      "step_3 INPUT_FROM step_2",
      "step_3 WITH_PARAM selector",
      "step_4 EXECUTES transform_json",
      "step_4 INPUT_FROM step_3",
      "step_5 EXECUTES save_file",
      "step_5 INPUT_FROM step_4",
      "step_5 WITH_PARAM filepath",
      "step_6 EXECUTES compress_archive",
      "step_6 INPUT_FROM step_5"
    ]
  },
  "queries": [
    {
      "id": "q1",
      "natural_language": "PLAN STEP 1: What is the first tool and what parameter does it need?",
      "expected_dsl": "@exec step_1 EXECUTES fetch_url\n@param step_1 WITH_PARAM url\n@q1 $exec AND $param",
      "expected_answer": {
        "dsl_explanation": "@e1 ASK step_1 EXECUTES fetch_url\n@p1 ASK step_1 WITH_PARAM url\n@step1_valid BOOL_AND $e1 $p1\n# Result: @step1_valid.truth = TRUE_CERTAIN proves: step_1 executes fetch_url with url parameter"
      }
    },
    {
      "id": "q2",
      "natural_language": "DEPENDENCY CHECK: What does parse_html need before it can run?",
      "expected_dsl": "@takes parse_html TAKES raw_html\n@produces fetch_url PRODUCES raw_html\n@q2 $takes AND $produces",
      "expected_answer": {
        "dsl_explanation": "@dep1 ASK parse_html TAKES raw_html\n@dep2 ASK fetch_url PRODUCES raw_html\n@chain BOOL_AND $dep1 $dep2\n# Result: @chain.truth = TRUE_CERTAIN proves: parse_html needs raw_html which fetch_url produces"
      }
    },
    {
      "id": "q3",
      "natural_language": "PLAN STEP 3: What inputs does extract_data need?",
      "expected_dsl": "@input1 extract_data TAKES dom_tree\n@input2 extract_data TAKES selector\n@q3 $input1 AND $input2",
      "expected_answer": {
        "dsl_explanation": "@i1 ASK extract_data TAKES dom_tree\n@i2 ASK extract_data TAKES selector\n@both_inputs BOOL_AND $i1 $i2\n# Result: @both_inputs.truth = TRUE_CERTAIN proves: extract_data takes dom_tree AND selector"
      }
    },
    {
      "id": "q4",
      "natural_language": "DATA FLOW: Verify the chain from step_2 to step_4",
      "expected_dsl": "@flow1 step_3 INPUT_FROM step_2\n@flow2 step_4 INPUT_FROM step_3\n@q4 $flow1 AND $flow2",
      "expected_answer": {
        "dsl_explanation": "@f1 ASK step_3 INPUT_FROM step_2\n@f2 ASK step_4 INPUT_FROM step_3\n@flow_valid BOOL_AND $f1 $f2\n# Result: @flow_valid.truth = TRUE_CERTAIN proves: step_2 → step_3 → step_4 data flow"
      }
    },
    {
      "id": "q5",
      "natural_language": "GOAL DECOMPOSITION: What sequence produces json_data?",
      "expected_dsl": "@req json_data REQUIRES_FIRST extracted_list\n@produces transform_json PRODUCES json_data\n@takes transform_json TAKES extracted_list\n@chain1 $req AND $produces\n@q5 $chain1 AND $takes",
      "expected_answer": {
        "dsl_explanation": "@r1 ASK json_data REQUIRES_FIRST extracted_list\n@r2 ASK transform_json PRODUCES json_data\n@r3 ASK transform_json TAKES extracted_list\n@c1 BOOL_AND $r1 $r2\n@full_chain BOOL_AND $c1 $r3\n# Result: @full_chain.truth = TRUE_CERTAIN proves: json_data needs extracted_list → transform_json"
      }
    },
    {
      "id": "q6",
      "natural_language": "PARAMETER MAPPING: Which steps need configuration parameters?",
      "expected_dsl": "@config1 step_1 WITH_PARAM url\n@config2 step_3 WITH_PARAM selector\n@config3 step_5 WITH_PARAM filepath\n@c1 $config1 AND $config2\n@q6 $c1 AND $config3",
      "expected_answer": {
        "dsl_explanation": "@p1 ASK step_1 WITH_PARAM url\n@p2 ASK step_3 WITH_PARAM selector\n@p3 ASK step_5 WITH_PARAM filepath\n@pair1 BOOL_AND $p1 $p2\n@all_params BOOL_AND $pair1 $p3\n# Result: @all_params.truth = TRUE_CERTAIN proves: url (step_1), selector (step_3), filepath (step_5)"
      }
    },
    {
      "id": "q7",
      "natural_language": "FINAL STEP: What produces the archive output?",
      "expected_dsl": "@produces compress_archive PRODUCES archive\n@takes compress_archive TAKES saved_file\n@q7 $produces AND $takes",
      "expected_answer": {
        "dsl_explanation": "@p1 ASK compress_archive PRODUCES archive\n@t1 ASK compress_archive TAKES saved_file\n@final BOOL_AND $p1 $t1\n# Result: @final.truth = TRUE_CERTAIN proves: compress_archive takes saved_file → produces archive"
      }
    },
    {
      "id": "q8",
      "natural_language": "FULL CHAIN: Verify the complete 6-step pipeline",
      "expected_dsl": "@s1 step_1 EXECUTES fetch_url\n@s2 step_2 EXECUTES parse_html\n@s3 step_3 EXECUTES extract_data\n@s4 step_4 EXECUTES transform_json\n@s5 step_5 EXECUTES save_file\n@s6 step_6 EXECUTES compress_archive\n@early $s1 AND $s2\n@mid1 $s3 AND $s4\n@mid2 $s5 AND $s6\n@half1 $early AND $mid1\n@q8 $half1 AND $mid2",
      "expected_answer": {
        "dsl_explanation": "@e1 ASK step_1 EXECUTES fetch_url\n@e2 ASK step_2 EXECUTES parse_html\n@e3 ASK step_3 EXECUTES extract_data\n@e4 ASK step_4 EXECUTES transform_json\n@e5 ASK step_5 EXECUTES save_file\n@e6 ASK step_6 EXECUTES compress_archive\n@p1 BOOL_AND $e1 $e2\n@p2 BOOL_AND $e3 $e4\n@p3 BOOL_AND $e5 $e6\n@h1 BOOL_AND $p1 $p2\n@full_pipeline BOOL_AND $h1 $p3\n# Result: @full_pipeline.truth = TRUE_CERTAIN proves 6-step chain: fetch_url → parse_html → extract_data → transform_json → save_file → compress_archive"
      }
    }
  ],
  "tags": [
    "tool_planning",
    "sequencing",
    "web_scraping",
    "pipeline",
    "data_flow",
    "multi_statement",
    "dsl_validatable"
  ],
  "version": "3.0"
}
